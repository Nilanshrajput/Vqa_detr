{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "DETR_Vqa_pytlightning",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyPuXe3E5eUdmslPCrEtN+Sx",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Nilanshrajput/Vqa_detr/blob/master/DETR_Vqa_pytlightning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kqe_0nc5dyAq",
        "colab_type": "text"
      },
      "source": [
        "# Object Detection with DETR - a minimal implementation\n",
        "\n",
        "In this notebook we show a demo of DETR (Detection Transformer), with slight differences with the baseline model in the paper.\n",
        "\n",
        "We show how to define the model, load pretrained weights and visualize bounding box and class predictions.\n",
        "\n",
        "Let's start with some common imports."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "saZ2BV_Ag0uc",
        "colab_type": "code",
        "outputId": "31cf31b0-14ee-41b8-d311-6e4235504d86",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 365
        }
      },
      "source": [
        "!pip install transformers"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.6/dist-packages (2.11.0)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.6/dist-packages (from transformers) (4.41.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.6/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.6/dist-packages (from transformers) (3.0.12)\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.6/dist-packages (from transformers) (0.0.43)\n",
            "Requirement already satisfied: dataclasses; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from transformers) (0.7)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: tokenizers==0.7.0 in /usr/local/lib/python3.6/dist-packages (from transformers) (0.7.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from transformers) (1.18.4)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.6/dist-packages (from transformers) (20.4)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.6/dist-packages (from transformers) (0.1.91)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (7.1.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (0.15.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (1.12.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2020.4.5.1)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2.9)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from packaging->transformers) (2.4.7)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jf59UNQ37QhJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from PIL import Image\n",
        "import requests\n",
        "import matplotlib.pyplot as plt\n",
        "%config InlineBackend.figure_format = 'retina'\n",
        "import json\n",
        "\n",
        "import torch\n",
        "import torch.utils.data as data\n",
        "from torch import nn\n",
        "from torchvision.models import resnet50\n",
        "import torchvision.transforms as T\n",
        "import os\n",
        "import json\n",
        "import tqdm\n",
        "\n",
        "import logging\n",
        "\n",
        "import numpy as np\n",
        "import torch\n",
        "from torch.utils.data import Dataset\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.autograd import Variable\n",
        "\n",
        "from transformers.tokenization_bert import BertTokenizer\n",
        "from transformers import BertConfig, EncoderDecoderConfig, EncoderDecoderModel, BertModel\n",
        "from transformers import AdamW\n",
        "from transformers import get_linear_schedule_with_warmup\n",
        "import torchtext\n",
        "\n",
        "import pdb\n",
        "import os"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h91rsIPl7tVl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class DETRdemo(nn.Module):\n",
        "    \"\"\"\n",
        "    Demo DETR implementation.\n",
        "\n",
        "    Demo implementation of DETR in minimal number of lines, with the\n",
        "    following differences wrt DETR in the paper:\n",
        "    * learned positional encoding (instead of sine)\n",
        "    * positional encoding is passed at input (instead of attention)\n",
        "    * fc bbox predictor (instead of MLP) nj\n",
        "    The model achieves ~40 AP on COCO val5k and runs at ~28 FPS on Tesla V100.\n",
        "    Only batch size 1 supported.\n",
        "    \"\"\"\n",
        "    def __init__(self, num_classes, hidden_dim=256, nheads=8,\n",
        "                 num_encoder_layers=6, num_decoder_layers=6):\n",
        "        super().__init__()\n",
        "\n",
        "        # create ResNet-50 backbone\n",
        "        self.backbone = resnet50()\n",
        "        del self.backbone.fc\n",
        "\n",
        "        # create conversion layer\n",
        "        self.conv = nn.Conv2d(2048, hidden_dim, 1)\n",
        "\n",
        "        # create a default PyTorch transformer\n",
        "        self.transformer = nn.Transformer(\n",
        "            hidden_dim, nheads, num_encoder_layers, num_decoder_layers)\n",
        "\n",
        "        # prediction heads, one extra class for predicting non-empty slots\n",
        "        # note that in baseline DETR linear_bbox layer is 3-layer MLP\n",
        "        self.linear_class = nn.Linear(hidden_dim, num_classes + 1)\n",
        "        self.linear_bbox = nn.Linear(hidden_dim, 4)\n",
        "\n",
        "        # output positional encodings (object queries)\n",
        "        self.query_pos = nn.Parameter(torch.rand(100, hidden_dim))\n",
        "\n",
        "        # spatial positional encodings\n",
        "        # note that in baseline DETR we use sine positional encodings\n",
        "        self.row_embed = nn.Parameter(torch.rand(50, hidden_dim // 2))\n",
        "        self.col_embed = nn.Parameter(torch.rand(50, hidden_dim // 2))\n",
        "\n",
        "    def forward(self, inputs):\n",
        "        # propagate inputs through ResNet-50 up to avg-pool layer\n",
        "        x = self.backbone.conv1(inputs)\n",
        "        x = self.backbone.bn1(x)\n",
        "        x = self.backbone.relu(x)\n",
        "        x = self.backbone.maxpool(x)\n",
        "\n",
        "        x = self.backbone.layer1(x)\n",
        "        x = self.backbone.layer2(x)\n",
        "        x = self.backbone.layer3(x)\n",
        "        x = self.backbone.layer4(x)\n",
        "\n",
        "        # convert from 2048 to 256 feature planes for the transformer\n",
        "        h = self.conv(x)\n",
        "        bb_ot = h\n",
        "        \n",
        "        # construct positional encodings\n",
        "        \"\"\"        H, W = h.shape[-2:]\n",
        "        pos = torch.cat([\n",
        "            self.col_embed[:W].unsqueeze(0).repeat(H, 1, 1),\n",
        "            self.row_embed[:H].unsqueeze(1).repeat(1, W, 1),\n",
        "        ], dim=-1).flatten(0, 1).unsqueeze(1)\"\"\"\n",
        "\n",
        "        bs,_,H, W = h.shape\n",
        "        pos = torch.cat([\n",
        "        self.col_embed[:W].unsqueeze(0).unsqueeze(1).repeat(bs,H, 1, 1),\n",
        "        self.row_embed[:H].unsqueeze(0).unsqueeze(2).repeat(bs,1, W, 1),\n",
        "        ], dim=-1).flatten(1, 2)\n",
        "\n",
        "\n",
        "        #print(self.col_embed[:W].unsqueeze(0).repeat(H, 1, 1))\n",
        "        # propagate through the transformer\n",
        "        #shape changed to (W*H,bs,hidden_dim) for both pos and h\n",
        "        h = self.transformer(pos.permute(1, 0, 2) + 0.1 * h.flatten(2).permute(2, 0, 1),\n",
        "                             self.query_pos.unsqueeze(1).repeat(1,bs,1)).transpose(0, 1)\n",
        "        \n",
        "        # finally project transformer outputs to class labels and bounding boxes\n",
        "        return {'pred_logits': self.linear_class(h), \n",
        "                'pred_boxes': self.linear_bbox(h).sigmoid(),\n",
        "                'decoder_out':h,\n",
        "                'res_out':bb_ot}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dComoNKgtMEN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class VQA_DETR(nn.Module):\n",
        "    def __init__(self,num_ans, hidden_size=256, num_attention_heads = 8, num_hidden_layers = 6):\n",
        "        super().__init__()\n",
        "\n",
        "        self.bert__decoder_config = BertConfig(is_decoder = True,hidden_size=hidden_size, num_attention_heads=num_attention_heads, num_hidden_layers=num_hidden_layers)\n",
        "        #self.enc_dec_config = EncoderDecoderConfig.from_encoder_decoder_configs(encoder_config= self.bert_config, decoder_config= self.bert_config)\n",
        "        #self.model = EncoderDecoderModel(config= self.enc_dec_config)\n",
        "        self.bert_decoder = BertModel(config=self.bert__decoder_config)\n",
        "\n",
        "        self.detr = DETRdemo(num_classes=91)\n",
        "        state_dict = torch.hub.load_state_dict_from_url(\n",
        "            url='https://dl.fbaipublicfiles.com/detr/detr_demo-da2a99e9.pth',\n",
        "            map_location='cpu', check_hash=True)\n",
        "        self.detr.load_state_dict(state_dict)\n",
        "        #self.detr  = self.detr.cuda()\n",
        "\n",
        "        self.classifier  = nn.Linear(hidden_size*2,num_ans)\n",
        "\n",
        "        self.drop_out = nn.Dropout(p=0.2)\n",
        "        self.log_softmax = nn.LogSoftmax().cuda()\n",
        "        \n",
        "\n",
        "    def forward(self,img, q_ids):\n",
        "        \n",
        "        img_ecs = self.detr(img)['decoder_out'].flatten(2)\n",
        "\n",
        "        #print(img_ecs.shape)#shape should be(bs. seQ_len, hiddensize)\n",
        "        #print(q_ids.shape)\n",
        "        #outputs = self.model(inputs_embeds=img_ecs, decoder_input_ids= q_ids )\n",
        "        o1,_ = self.bert_decoder(input_ids = q_ids, encoder_hidden_states = img_ecs)\n",
        "\n",
        "        mean_pool = torch.mean(o1,1)\n",
        "        max_pool,_ = torch.max(o1,1)\n",
        "        cat = torch.cat((mean_pool, max_pool),1)\n",
        "\n",
        "        bo = self.drop_out(cat)\n",
        "        output = self.classifier(bo)\n",
        "        \n",
        "       \n",
        "        nll = -self.log_softmax(output)\n",
        "\n",
        "        return {'logits':output,\n",
        "                'nll':nll}\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NiPadTPQbG0u",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "def assert_eq(real, expected):\n",
        "    assert real == expected, \"%s (true) vs %s (expected)\" % (real, expected)\n",
        "\n",
        "def _create_entry(question, answer):\n",
        "    answer.pop(\"image_id\")\n",
        "    answer.pop(\"question_id\")\n",
        "    entry = {\n",
        "        \"question_id\": question[\"question_id\"],\n",
        "        \"image_id\": question[\"image_id\"],\n",
        "        \"question\": question[\"question\"],\n",
        "        \"answer\": [a['answer'] for a in answer['answers']],\n",
        "    }\n",
        "    return entry\n",
        "\n",
        "def _load_dataset(dataroot, name):\n",
        "    \"\"\"Load entries\n",
        "    dataroot: root path of dataset\n",
        "    name: 'train', 'val', 'trainval', 'minsval'\n",
        "    \"\"\"\n",
        "    if name == 'train' or name == 'val':\n",
        "        question_path = os.path.join(dataroot, \"v2_OpenEnded_mscoco_%s2014_questions.json\" % name)\n",
        "        questions = sorted(json.load(open(question_path))[\"questions\"], key=lambda x: x[\"question_id\"])\n",
        "        answer_path = os.path.join(dataroot, \"v2_mscoco_%s2014_annotations.json\" % name)\n",
        "        answers = json.load(open(answer_path, \"rb\"))[\"annotations\"]\n",
        "        answers = sorted(answers, key=lambda x: x[\"question_id\"])\n",
        "\n",
        "    elif name  == 'trainval':\n",
        "        question_path_train = os.path.join(dataroot, \"v2_OpenEnded_mscoco_%s2014_questions.json\" % 'train')\n",
        "        questions_train = sorted(json.load(open(question_path_train))[\"questions\"], key=lambda x: x[\"question_id\"])\n",
        "        answer_path_train = os.path.join(dataroot, \"v2_mscoco_%s2014_annotations.json\" % 'train')\n",
        "        answers_train = json.load(open(answer_path_train, \"rb\"))[\"annotations\"]\n",
        "        answers_train = sorted(answers_train, key=lambda x: x[\"question_id\"])\n",
        "\n",
        "        question_path_val = os.path.join(dataroot, \"v2_OpenEnded_mscoco_%s2014_questions.json\" % 'val')\n",
        "        questions_val = sorted(json.load(open(question_path_val))[\"questions\"], key=lambda x: x[\"question_id\"])\n",
        "        answer_path_val = os.path.join(dataroot, \"v2_mscoco_%s2014_annotations.json\" % 'val')\n",
        "        answers_val = json.load(open(answer_path_val, \"rb\"))[\"annotations\"]\n",
        "        answers_val = sorted(answers_val, key=lambda x: x[\"question_id\"])\n",
        "        questions = questions_train + questions_val[:-3000]\n",
        "        answers = answers_train + answers_val[:-3000]\n",
        "\n",
        "    elif name == 'minval':\n",
        "        question_path_val = os.path.join(dataroot, \"v2_OpenEnded_mscoco_%s2014_questions.json\" % 'val')\n",
        "        questions_val = sorted(json.load(open(question_path_val))[\"questions\"], key=lambda x: x[\"question_id\"])\n",
        "        answer_path_val = os.path.join(dataroot, \"v2_mscoco_%s2014_annotations.json\" % 'val')\n",
        "        answers_val = json.load(open(answer_path_val, \"rb\"))[\"annotations\"]\n",
        "        answers_val = sorted(answers_val, key=lambda x: x[\"question_id\"])        \n",
        "        questions = questions_val[-3000:]\n",
        "        answers = answers_val[-3000:]\n",
        "\n",
        "    elif name == 'test':\n",
        "        question_path_test = os.path.join(dataroot, \"v2_OpenEnded_mscoco_%s2015_questions.json\" % 'test')\n",
        "        questions_test = sorted(json.load(open(question_path_test))[\"questions\"], key=lambda x: x[\"question_id\"])\n",
        "        questions = questions_test\n",
        "    else:\n",
        "        assert False, \"data split is not recognized.\"\n",
        "\n",
        "    if 'test' in name:\n",
        "        entries = []\n",
        "        for question in questions:\n",
        "            entries.append(question)\n",
        "    else:\n",
        "        assert_eq(len(questions), len(answers))\n",
        "        entries = []\n",
        "        for question, answer in zip(questions, answers):\n",
        "            assert_eq(question[\"question_id\"], answer[\"question_id\"])\n",
        "            assert_eq(question[\"image_id\"], answer[\"image_id\"])\n",
        "            entries.append(_create_entry(question, answer))\n",
        "    return entries"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c_KSLg6prP4J",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "entries = _load_dataset(dataroot='/content/',name='train')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tdewFhEzUU1d",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# compile a list of all the answers\n",
        "all_answers  = set()\n",
        "for a in entries:\n",
        "    all_answers.update(a['answer'])\n",
        "all_answers=list(all_answers)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d-Q_quxQmxOv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "answer_to_index = dict()\n",
        "for i,answer in enumerate(all_answers):\n",
        "    answer_to_index[answer]=i\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JUKQ1uLmfT4h",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "vqa_detr = VQA_DETR(num_ans=len(answer_to_index))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BBlfQ2YngeTd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class VQA(data.Dataset):\n",
        "    \"\"\" VQA dataset, open-ended \"\"\"\n",
        "    def __init__(self, root, answer_to_index, tokenizer ,split = 'train', max_len = 20):\n",
        "        super(VQA, self).__init__()\n",
        "\n",
        "\n",
        "        self.root = root\n",
        "        self.answer_to_index = answer_to_index\n",
        "        self.split = split\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_len = max_len\n",
        "        self.entries = self._load_dataset( self.root, self.split)\n",
        "\n",
        "         # standard PyTorch mean-std input image normalization\n",
        "        self.transform = T.Compose([\n",
        "            T.Resize(size=(800,800)),\n",
        "            T.ToTensor(),\n",
        "            T.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
        "        ])\n",
        "\n",
        "        self.id_to_image_fname = self._find_iamges()\n",
        "\n",
        "\n",
        "    def assert_eq(self,real, expected):\n",
        "        assert real == expected, \"%s (true) vs %s (expected)\" % (real, expected)\n",
        "\n",
        "    def _create_entry(self,question, answer):\n",
        "        answer.pop(\"image_id\")\n",
        "        answer.pop(\"question_id\")\n",
        "        entry = {\n",
        "            \"question_id\": question[\"question_id\"],\n",
        "            \"image_id\": question[\"image_id\"],\n",
        "            \"question\": question[\"question\"],\n",
        "            \"answer\": [a['answer'] for a in answer['answers']],\n",
        "        }\n",
        "        return entry\n",
        "\n",
        "    def _load_dataset(self,dataroot, name):\n",
        "        \"\"\"Load entries\n",
        "        dataroot: root path of dataset\n",
        "        name: 'train', 'val', 'trainval', 'minsval'\n",
        "        \"\"\"\n",
        "        if name == 'train' or name == 'val':\n",
        "            question_path = os.path.join(dataroot, \"v2_OpenEnded_mscoco_%s2014_questions.json\" % name)\n",
        "            questions = sorted(json.load(open(question_path))[\"questions\"], key=lambda x: x[\"question_id\"])\n",
        "            answer_path = os.path.join(dataroot, \"v2_mscoco_%s2014_annotations.json\" % name)\n",
        "            answers = json.load(open(answer_path, \"rb\"))[\"annotations\"]\n",
        "            answers = sorted(answers, key=lambda x: x[\"question_id\"])\n",
        "\n",
        "        elif name  == 'trainval':\n",
        "            question_path_train = os.path.join(dataroot, \"v2_OpenEnded_mscoco_%s2014_questions.json\" % 'train')\n",
        "            questions_train = sorted(json.load(open(question_path_train))[\"questions\"], key=lambda x: x[\"question_id\"])\n",
        "            answer_path_train = os.path.join(dataroot, \"v2_mscoco_%s2014_annotations.json\" % 'train')\n",
        "            answers_train = json.load(open(answer_path_train, \"rb\"))[\"annotations\"]\n",
        "            answers_train = sorted(answers_train, key=lambda x: x[\"question_id\"])\n",
        "\n",
        "            question_path_val = os.path.join(dataroot, \"v2_OpenEnded_mscoco_%s2014_questions.json\" % 'val')\n",
        "            questions_val = sorted(json.load(open(question_path_val))[\"questions\"], key=lambda x: x[\"question_id\"])\n",
        "            answer_path_val = os.path.join(dataroot, \"v2_mscoco_%s2014_annotations.json\" % 'val')\n",
        "            answers_val = json.load(open(answer_path_val, \"rb\"))[\"annotations\"]\n",
        "            answers_val = sorted(answers_val, key=lambda x: x[\"question_id\"])\n",
        "            questions = questions_train + questions_val[:-3000]\n",
        "            answers = answers_train + answers_val[:-3000]\n",
        "\n",
        "        elif name == 'minval':\n",
        "            question_path_val = os.path.join(dataroot, \"v2_OpenEnded_mscoco_%s2014_questions.json\" % 'val')\n",
        "            questions_val = sorted(json.load(open(question_path_val))[\"questions\"], key=lambda x: x[\"question_id\"])\n",
        "            answer_path_val = os.path.join(dataroot, \"v2_mscoco_%s2014_annotations.json\" % 'val')\n",
        "            answers_val = json.load(open(answer_path_val, \"rb\"))[\"annotations\"]\n",
        "            answers_val = sorted(answers_val, key=lambda x: x[\"question_id\"])        \n",
        "            questions = questions_val[-3000:]\n",
        "            answers = answers_val[-3000:]\n",
        "\n",
        "        elif name == 'test':\n",
        "            question_path_test = os.path.join(dataroot, \"v2_OpenEnded_mscoco_%s2015_questions.json\" % 'test')\n",
        "            questions_test = sorted(json.load(open(question_path_test))[\"questions\"], key=lambda x: x[\"question_id\"])\n",
        "            questions = questions_test\n",
        "        else:\n",
        "            assert False, \"data split is not recognized.\"\n",
        "\n",
        "        if 'test' in name:\n",
        "            entries = []\n",
        "            for question in questions:\n",
        "                entries.append(question)\n",
        "        else:\n",
        "            assert_eq(len(questions), len(answers))\n",
        "            entries = []\n",
        "            for question, answer in zip(questions, answers):\n",
        "                assert_eq(question[\"question_id\"], answer[\"question_id\"])\n",
        "                assert_eq(question[\"image_id\"], answer[\"image_id\"])\n",
        "                entries.append(_create_entry(question, answer))\n",
        "        return entries\n",
        "\n",
        "\n",
        "\n",
        "    def _encode_question(self, question):\n",
        "        \"\"\" Turn a question into a vector of indices and a question length \"\"\"\n",
        "        \n",
        "        inputs = self.tokenizer.encode_plus(\n",
        "            question,\n",
        "            None,\n",
        "            add_special_tokens=True,\n",
        "            max_length=self.max_len,\n",
        "            )\n",
        "\n",
        "        ids = inputs[\"input_ids\"]\n",
        "        mask = inputs[\"attention_mask\"]\n",
        "        token_type_ids = inputs[\"token_type_ids\"]\n",
        "\n",
        "        padding_length = self.max_len - len(ids)\n",
        "        ids += ([0]*padding_length)\n",
        "        mask += ([0]*padding_length)\n",
        "        token_type_ids += ([0]*padding_length)\n",
        "        \n",
        "        return {\n",
        "            'ids': torch.tensor(ids, dtype=torch.long),\n",
        "            'mask': torch.tensor(mask, dtype=torch.long),\n",
        "            'token_type_ids': torch.tensor(token_type_ids, dtype=torch.long),\n",
        "            \n",
        "        }\n",
        "\n",
        "    def _encode_ansfor_bert(self, answers):\n",
        "        pass\n",
        "\n",
        "\n",
        "    def _encode_answers(self, answers):\n",
        "        \"\"\" Turn an answer into a vector \"\"\"\n",
        "        # answer vec will be a vector of answer counts to determine which answers will contribute to the loss.\n",
        "        # this should be multiplied with 0.1 * negative log-likelihoods that a model produces and then summed up\n",
        "        # to get the loss that is weighted by how many humans gave that answer\n",
        "        answer_vec = torch.zeros(len(self.answer_to_index),dtype=torch.long)\n",
        "        for answer in answers:\n",
        "            index = self.answer_to_index.get(answer)\n",
        "            if index is not None:\n",
        "                answer_vec[index] += 1\n",
        "        _,idx = answer_vec.max(dim = 0)\n",
        "        idx = torch.tensor(idx, dtype = torch.long)\n",
        "\n",
        "        return idx\n",
        "\n",
        "   \n",
        "    def _find_iamges(self):\n",
        "        id_to_filename = {}\n",
        "        imgs_folder = os.path.join(self.root,'train2014')\n",
        "        for filename in os.listdir(imgs_folder):\n",
        "            if not filename.endswith('.jpg'):\n",
        "                continue\n",
        "            id_and_extension = filename.split('_')[-1]\n",
        "            id = int(id_and_extension.split('.')[0])\n",
        "            id_to_filename[id] = os.path.join(imgs_folder,filename)\n",
        "        return id_to_filename\n",
        "\n",
        "\n",
        "    def _load_image(self, image_id):\n",
        "        \"\"\" Load an image \"\"\"\n",
        "\n",
        "        img_path = self.id_to_image_fname[image_id]\n",
        "        img  = Image.open(img_path)\n",
        "        img = np.asarray(img)\n",
        "        \n",
        "        if len(img.shape)==2:\n",
        "            print(img.shape)\n",
        "            img=np.expand_dims(img, axis=-1)\n",
        "            \n",
        "            img = np.repeat(img,3, axis = -1)\n",
        "            print(img.shape)\n",
        "\n",
        "        return img\n",
        "\n",
        "    def __getitem__(self, item):\n",
        "       \n",
        "        entry  = self.entries[item]\n",
        "        q = entry['question']\n",
        "        a = self._encode_answers(entry['answer'])\n",
        "        image_id = entry['image_id']\n",
        "\n",
        "        img = self._load_image(image_id)\n",
        "        img = Image.fromarray(img)\n",
        "        img = self.transform(img)\n",
        "        #question_id = entry['question_id']\n",
        "        q= self._encode_question(q)\n",
        "\n",
        "        return img, q, a\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.entries)\n",
        "\n",
        "    @staticmethod\n",
        "    def collate_fn(batch):\n",
        "        \"\"\"The collat_fn method to be used by the\n",
        "        PyTorch data loader.\n",
        "        \"\"\"\n",
        "        \n",
        "        # Unzip the batch\n",
        "\n",
        "        imgs,qs, answers = list(zip(*batch))\n",
        "\n",
        "        # concatenate the vectors\n",
        "        imgs = torch.stack(imgs)\n",
        "        \n",
        "        #concatenate the labels\n",
        "        q = torch.stack(qs)\n",
        "\n",
        "        a = torch.stack(answers)\n",
        "        \n",
        "        return imgs, q, a"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ff9kOL6KIncK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "bert_tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rhYh98-8GROJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "vqa_data = VQA(root='/content', answer_to_index=answer_to_index,split= 'train', tokenizer=bert_tokenizer, max_len=15 )"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cXeflNl8HEy7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "data_loader = DataLoader(vqa_data, batch_size = 4, shuffle= True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "trgslo7xg5Rz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "def loss_fn(outputs, targets):\n",
        "    \n",
        "    #print(outputs.shape)\n",
        "    with torch.enable_grad():\n",
        "        loss  = nn.CrossEntropyLoss()(outputs, targets)\n",
        "        loss.requres_grad = True\n",
        "    \n",
        "   \n",
        "    return loss\n",
        "\n",
        "\n",
        "def train_fn(data_loader, model, optimizer, device, scheduler):\n",
        "    model.train()\n",
        "    loss_v=0\n",
        "    for bi, data in tqdm.tqdm(enumerate(data_loader), total=len(data_loader)):\n",
        "        im,q,a  = data\n",
        "        ids = q[\"ids\"]\n",
        "        \n",
        "        targets = a\n",
        "\n",
        "        ids = ids.to(device, dtype=torch.long)\n",
        "        targets = targets.to(device, dtype=torch.long )\n",
        "        im = im.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(\n",
        "            im,ids\n",
        "        )\n",
        "\n",
        "        output =outputs['logits'].to(device)\n",
        "        #print(output.dtype)\n",
        "\n",
        "        \n",
        "        #a,targets=torch.max(output,dim = 1)\n",
        "        #targets = targets.to(device, dtype=torch.long)\n",
        "        #targets = Variable(targets)\n",
        " \n",
        "       \n",
        "        loss = loss_fn(output, targets)\n",
        "        loss.backward()\n",
        "        loss_v+= loss.item()\n",
        "        print(loss.item())\n",
        "        optimizer.step()\n",
        "        scheduler.step()\n",
        "\n",
        "    return loss_v\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X-Yy86DamBUV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "bs = 16\n",
        "epochs = 10"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Whjkrr8Ag5Oy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "device = torch.device(\"cuda\")\n",
        "model = vqa_detr\n",
        "for param in model.parameters():\n",
        "    param.requires_grad = True\n",
        "model.to(device)\n",
        "\n",
        "\n",
        "param_optimizer = list(model.named_parameters())\n",
        "no_decay = [\"bias\", \"LayerNorm.bias\", \"LayerNorm.weight\"]\n",
        "optimizer_parameters = [\n",
        "    {'params': [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)], 'weight_decay': 0.001},\n",
        "    {'params': [p for n, p in param_optimizer if any(nd in n for nd in no_decay)], 'weight_decay': 0.0},\n",
        "]\n",
        "\n",
        "num_train_steps = int(len(vqa_data )/ bs * epochs)\n",
        "optimizer = AdamW(optimizer_parameters, lr=3e-5)\n",
        "scheduler = get_linear_schedule_with_warmup(\n",
        "    optimizer,\n",
        "    num_warmup_steps=0,\n",
        "    num_training_steps=num_train_steps\n",
        ")\n",
        "\n",
        "\n",
        "\n",
        "for epoch in range(epochs):\n",
        "    loss = train_fn(data_loader, model, optimizer, device, scheduler)\n",
        "\n",
        "    print(f'The loss for epoch {epoch} = {loss}')\n",
        "\n",
        "    "
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}