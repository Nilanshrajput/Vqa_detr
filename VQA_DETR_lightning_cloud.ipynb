{"cells":[{"metadata":{"id":"view-in-github"},"cell_type":"markdown","source":["<a href=\"https://colab.research.google.com/github/Nilanshrajput/Vqa_detr/blob/master/DETR_Vqa_pytlightning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"]},{"metadata":{"id":"kqe_0nc5dyAq"},"cell_type":"markdown","source":["# Object Detection with DETR - a minimal implementation\n","\n","In this notebook we show a demo of DETR (Detection Transformer), with slight differences with the baseline model in the paper.\n","\n","We show how to define the model, load pretrained weights and visualize bounding box and class predictions.\n","\n","Let's start with some common imports."]},{"metadata":{"id":"saZ2BV_Ag0uc","outputId":"69f145ad-b922-4da4-83e2-062ba90c5fd1","trusted":true},"cell_type":"code","source":["!pip install transformers\n","!pip install pytorch-lightning"],"execution_count":2,"outputs":[{"output_type":"stream","text":"Requirement already satisfied: transformers in /opt/conda/lib/python3.7/site-packages (2.11.0)\nRequirement already satisfied: sacremoses in /opt/conda/lib/python3.7/site-packages (from transformers) (0.0.43)\nRequirement already satisfied: packaging in /opt/conda/lib/python3.7/site-packages (from transformers) (20.1)\nRequirement already satisfied: requests in /opt/conda/lib/python3.7/site-packages (from transformers) (2.23.0)\nRequirement already satisfied: sentencepiece in /opt/conda/lib/python3.7/site-packages (from transformers) (0.1.91)\nRequirement already satisfied: tokenizers==0.7.0 in /opt/conda/lib/python3.7/site-packages (from transformers) (0.7.0)\nRequirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.7/site-packages (from transformers) (2020.4.4)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.7/site-packages (from transformers) (3.0.10)\nRequirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.7/site-packages (from transformers) (4.45.0)\nRequirement already satisfied: numpy in /opt/conda/lib/python3.7/site-packages (from transformers) (1.18.1)\nRequirement already satisfied: six in /opt/conda/lib/python3.7/site-packages (from sacremoses->transformers) (1.14.0)\nRequirement already satisfied: click in /opt/conda/lib/python3.7/site-packages (from sacremoses->transformers) (7.1.1)\nRequirement already satisfied: joblib in /opt/conda/lib/python3.7/site-packages (from sacremoses->transformers) (0.14.1)\nRequirement already satisfied: pyparsing>=2.0.2 in /opt/conda/lib/python3.7/site-packages (from packaging->transformers) (2.4.7)\nRequirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /opt/conda/lib/python3.7/site-packages (from requests->transformers) (1.24.3)\nRequirement already satisfied: chardet<4,>=3.0.2 in /opt/conda/lib/python3.7/site-packages (from requests->transformers) (3.0.4)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.7/site-packages (from requests->transformers) (2020.4.5.1)\nRequirement already satisfied: idna<3,>=2.5 in /opt/conda/lib/python3.7/site-packages (from requests->transformers) (2.9)\nCollecting pytorch-lightning\n  Downloading pytorch_lightning-0.7.6-py3-none-any.whl (248 kB)\n\u001b[K     |████████████████████████████████| 248 kB 2.8 MB/s eta 0:00:01\n\u001b[?25hRequirement already satisfied: torch>=1.1 in /opt/conda/lib/python3.7/site-packages (from pytorch-lightning) (1.5.0)\nRequirement already satisfied: tensorboard>=1.14 in /opt/conda/lib/python3.7/site-packages (from pytorch-lightning) (2.2.2)\nRequirement already satisfied: numpy>=1.16.4 in /opt/conda/lib/python3.7/site-packages (from pytorch-lightning) (1.18.1)\nRequirement already satisfied: pyyaml>=3.13 in /opt/conda/lib/python3.7/site-packages (from pytorch-lightning) (5.3.1)\nRequirement already satisfied: future>=0.17.1 in /opt/conda/lib/python3.7/site-packages (from pytorch-lightning) (0.18.2)\nRequirement already satisfied: tqdm>=4.41.0 in /opt/conda/lib/python3.7/site-packages (from pytorch-lightning) (4.45.0)\nRequirement already satisfied: absl-py>=0.4 in /opt/conda/lib/python3.7/site-packages (from tensorboard>=1.14->pytorch-lightning) (0.9.0)\nRequirement already satisfied: markdown>=2.6.8 in /opt/conda/lib/python3.7/site-packages (from tensorboard>=1.14->pytorch-lightning) (3.2.1)\nRequirement already satisfied: setuptools>=41.0.0 in /opt/conda/lib/python3.7/site-packages (from tensorboard>=1.14->pytorch-lightning) (46.1.3.post20200325)\nRequirement already satisfied: werkzeug>=0.11.15 in /opt/conda/lib/python3.7/site-packages (from tensorboard>=1.14->pytorch-lightning) (1.0.1)\nRequirement already satisfied: google-auth<2,>=1.6.3 in /opt/conda/lib/python3.7/site-packages (from tensorboard>=1.14->pytorch-lightning) (1.14.0)\nRequirement already satisfied: wheel>=0.26; python_version >= \"3\" in /opt/conda/lib/python3.7/site-packages (from tensorboard>=1.14->pytorch-lightning) (0.34.2)\nRequirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /opt/conda/lib/python3.7/site-packages (from tensorboard>=1.14->pytorch-lightning) (0.4.1)\nRequirement already satisfied: requests<3,>=2.21.0 in /opt/conda/lib/python3.7/site-packages (from tensorboard>=1.14->pytorch-lightning) (2.23.0)\nRequirement already satisfied: protobuf>=3.6.0 in /opt/conda/lib/python3.7/site-packages (from tensorboard>=1.14->pytorch-lightning) (3.11.4)\nRequirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /opt/conda/lib/python3.7/site-packages (from tensorboard>=1.14->pytorch-lightning) (1.6.0.post3)\nRequirement already satisfied: grpcio>=1.24.3 in /opt/conda/lib/python3.7/site-packages (from tensorboard>=1.14->pytorch-lightning) (1.29.0)\nRequirement already satisfied: six>=1.10.0 in /opt/conda/lib/python3.7/site-packages (from tensorboard>=1.14->pytorch-lightning) (1.14.0)\nRequirement already satisfied: cachetools<5.0,>=2.0.0 in /opt/conda/lib/python3.7/site-packages (from google-auth<2,>=1.6.3->tensorboard>=1.14->pytorch-lightning) (3.1.1)\nRequirement already satisfied: rsa<4.1,>=3.1.4 in /opt/conda/lib/python3.7/site-packages (from google-auth<2,>=1.6.3->tensorboard>=1.14->pytorch-lightning) (4.0)\nRequirement already satisfied: pyasn1-modules>=0.2.1 in /opt/conda/lib/python3.7/site-packages (from google-auth<2,>=1.6.3->tensorboard>=1.14->pytorch-lightning) (0.2.7)\nRequirement already satisfied: requests-oauthlib>=0.7.0 in /opt/conda/lib/python3.7/site-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard>=1.14->pytorch-lightning) (1.2.0)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.7/site-packages (from requests<3,>=2.21.0->tensorboard>=1.14->pytorch-lightning) (2020.4.5.1)\nRequirement already satisfied: chardet<4,>=3.0.2 in /opt/conda/lib/python3.7/site-packages (from requests<3,>=2.21.0->tensorboard>=1.14->pytorch-lightning) (3.0.4)\nRequirement already satisfied: idna<3,>=2.5 in /opt/conda/lib/python3.7/site-packages (from requests<3,>=2.21.0->tensorboard>=1.14->pytorch-lightning) (2.9)\nRequirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /opt/conda/lib/python3.7/site-packages (from requests<3,>=2.21.0->tensorboard>=1.14->pytorch-lightning) (1.24.3)\nRequirement already satisfied: pyasn1>=0.1.3 in /opt/conda/lib/python3.7/site-packages (from rsa<4.1,>=3.1.4->google-auth<2,>=1.6.3->tensorboard>=1.14->pytorch-lightning) (0.4.8)\nRequirement already satisfied: oauthlib>=3.0.0 in /opt/conda/lib/python3.7/site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard>=1.14->pytorch-lightning) (3.0.1)\nInstalling collected packages: pytorch-lightning\nSuccessfully installed pytorch-lightning-0.7.6\n","name":"stdout"}]},{"metadata":{"id":"Jf59UNQ37QhJ","trusted":true},"cell_type":"code","source":["from PIL import Image\n","import requests\n","import matplotlib.pyplot as plt\n","%config InlineBackend.figure_format = 'retina'\n","import json\n","\n","import torch\n","import torch.utils.data as data\n","from torch import nn\n","from torchvision.models import resnet50\n","import torchvision.transforms as T\n","import os\n","import json\n","import tqdm\n","\n","import logging\n","from argparse import Namespace\n","\n","from functools import lru_cache\n","\n","import numpy as np\n","import torch\n","from torch.utils.data import Dataset\n","from torch.utils.data import DataLoader\n","from torch.autograd import Variable\n","\n","from transformers.tokenization_bert import BertTokenizer\n","from transformers import BertConfig, EncoderDecoderConfig, EncoderDecoderModel, BertModel\n","from transformers import AdamW\n","from transformers import get_linear_schedule_with_warmup\n","import torchtext\n","\n","from pytorch_lightning.core.lightning import LightningModule\n","from pytorch_lightning import Trainer\n","\n","\n","\n","import pdb\n","import os"],"execution_count":3,"outputs":[{"output_type":"stream","text":"\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m W&B installed but not logged in.  Run `wandb login` or set the WANDB_API_KEY env variable.\n","name":"stderr"}]},{"metadata":{"id":"RSnU5JFxGeDe"},"cell_type":"markdown","source":["## DETR\n","Here is a minimal implementation of DETR:"]},{"metadata":{"id":"h91rsIPl7tVl","trusted":true},"cell_type":"code","source":["class DETRdemo(nn.Module):\n","    \"\"\"\n","    Demo DETR implementation.\n","\n","    Demo implementation of DETR in minimal number of lines, with the\n","    following differences wrt DETR in the paper:\n","    * learned positional encoding (instead of sine)\n","    * positional encoding is passed at input (instead of attention)\n","    * fc bbox predictor (instead of MLP) nj\n","    The model achieves ~40 AP on COCO val5k and runs at ~28 FPS on Tesla V100.\n","    Only batch size 1 supported.\n","    \"\"\"\n","    def __init__(self, num_classes, hidden_dim=256, nheads=8,\n","                 num_encoder_layers=6, num_decoder_layers=6):\n","        super().__init__()\n","\n","        # create ResNet-50 backbone\n","        self.backbone = resnet50()\n","        del self.backbone.fc\n","\n","        # create conversion layer\n","        self.conv = nn.Conv2d(2048, hidden_dim, 1)\n","\n","        # create a default PyTorch transformer\n","        self.transformer = nn.Transformer(\n","            hidden_dim, nheads, num_encoder_layers, num_decoder_layers)\n","\n","        # prediction heads, one extra class for predicting non-empty slots\n","        # note that in baseline DETR linear_bbox layer is 3-layer MLP\n","        self.linear_class = nn.Linear(hidden_dim, num_classes + 1)\n","        self.linear_bbox = nn.Linear(hidden_dim, 4)\n","\n","        # output positional encodings (object queries)\n","        self.query_pos = nn.Parameter(torch.rand(100, hidden_dim))\n","\n","        # spatial positional encodings\n","        # note that in baseline DETR we use sine positional encodings\n","        self.row_embed = nn.Parameter(torch.rand(50, hidden_dim // 2))\n","        self.col_embed = nn.Parameter(torch.rand(50, hidden_dim // 2))\n","\n","    def forward(self, inputs):\n","        # propagate inputs through ResNet-50 up to avg-pool layer\n","        x = self.backbone.conv1(inputs)\n","        x = self.backbone.bn1(x)\n","        x = self.backbone.relu(x)\n","        x = self.backbone.maxpool(x)\n","\n","        x = self.backbone.layer1(x)\n","        x = self.backbone.layer2(x)\n","        x = self.backbone.layer3(x)\n","        x = self.backbone.layer4(x)\n","\n","        # convert from 2048 to 256 feature planes for the transformer\n","        h = self.conv(x)\n","        bb_ot = h\n","        \n","        # construct positional encodings\n","        \"\"\"        H, W = h.shape[-2:]\n","        pos = torch.cat([\n","            self.col_embed[:W].unsqueeze(0).repeat(H, 1, 1),\n","            self.row_embed[:H].unsqueeze(1).repeat(1, W, 1),\n","        ], dim=-1).flatten(0, 1).unsqueeze(1)\"\"\"\n","\n","        bs,_,H, W = h.shape\n","        pos = torch.cat([\n","        self.col_embed[:W].unsqueeze(0).unsqueeze(1).repeat(bs,H, 1, 1),\n","        self.row_embed[:H].unsqueeze(0).unsqueeze(2).repeat(bs,1, W, 1),\n","        ], dim=-1).flatten(1, 2)\n","\n","\n","        #print(self.col_embed[:W].unsqueeze(0).repeat(H, 1, 1))\n","        # propagate through the transformer\n","        #shape changed to (W*H,bs,hidden_dim) for both pos and h\n","        h = self.transformer(pos.permute(1, 0, 2) + 0.1 * h.flatten(2).permute(2, 0, 1),\n","                             self.query_pos.unsqueeze(1).repeat(1,bs,1)).transpose(0, 1)\n","        \n","        # finally project transformer outputs to class labels and bounding boxes\n","        return {'pred_logits': self.linear_class(h), \n","                'pred_boxes': self.linear_bbox(h).sigmoid(),\n","                'decoder_out':h,\n","                'res_out':bb_ot}"],"execution_count":4,"outputs":[]},{"metadata":{"id":"dComoNKgtMEN","trusted":true},"cell_type":"code","source":["class VQA_DETR(LightningModule):\n","    def __init__(self,hparams,num_ans,ans_to_index,hidden_size=256, num_attention_heads = 8, num_hidden_layers = 6):\n","        super().__init__()\n","\n","        self.hparams = hparams\n","        self.tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)\n","        self.ans_to_index = ans_to_index\n","        self.bert_decoder_config = BertConfig(is_decoder = True,hidden_size=hidden_size, num_attention_heads=num_attention_heads, num_hidden_layers=num_hidden_layers)\n","        #self.enc_dec_config = EncoderDecoderConfig.from_encoder_decoder_configs(encoder_config= self.bert_config, decoder_config= self.bert_config)\n","        #self.model = EncoderDecoderModel(config= self.enc_dec_config)\n","        self.bert_decoder = BertModel(config=self.bert_decoder_config)\n","\n","        self.detr = DETRdemo(num_classes=91)\n","        state_dict = torch.hub.load_state_dict_from_url(\n","            url='https://dl.fbaipublicfiles.com/detr/detr_demo-da2a99e9.pth',\n","            map_location='cpu', check_hash=True)\n","        self.detr.load_state_dict(state_dict)\n","        del state_dict\n","        #self.detr  = self.detr.cuda()\n","\n","        self.classifier  = nn.Linear(hidden_size*2,num_ans)\n","\n","        self.drop_out = nn.Dropout(p=0.2)\n","        self.log_softmax = nn.LogSoftmax().cuda()\n","        \n","\n","    def forward(self,img, q_ids):\n","        \n","        img_ecs = self.detr(img)['decoder_out'].flatten(2)\n","        o1,_ = self.bert_decoder(input_ids = q_ids, encoder_hidden_states = img_ecs)\n","\n","        mean_pool = torch.mean(o1,1)\n","        max_pool,_ = torch.max(o1,1)\n","        cat = torch.cat((mean_pool, max_pool),1)\n","\n","        bo = self.drop_out(cat)\n","        output = self.classifier(bo)\n","        \n","        nll = -self.log_softmax(output)\n","\n","        return {'logits':output,'nll':nll}\n","\n","    def training_step(self, batch, batch_idx):\n","        im,q,a  = batch\n","        ids = q[\"ids\"]\n","\n","        outputs = self(im,ids)\n","        output =outputs['nll']\n","\n","        loss = self.loss_fn(output, a)\n","        tensorboard_logs = {'train_loss': loss}\n","\n","        return {'loss': loss, 'log': tensorboard_logs}\n","\n","    def loss_fn(self, nll, targets):\n","\n","        return (nll * targets / 10).sum(dim=1).mean()#nn.CrossEntropyLoss()(outputs, targets)\n","\n","    @lru_cache()\n","    def total_steps(self):\n","        return len(self.train_dataloader()) // self.hparams.accumulate_grad_batches * self.hparams.epochs\n","\n","\n","    def configure_optimizers(self):\n","        param_optimizer = list(self.named_parameters())\n","        no_decay = [\"bias\", \"LayerNorm.bias\", \"LayerNorm.weight\"]\n","        optimizer_parameters = [\n","            {'params': [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)], 'weight_decay': 0.001},\n","            {'params': [p for n, p in param_optimizer if any(nd in n for nd in no_decay)], 'weight_decay': 0.0},\n","        ]\n","\n","        optimizer = AdamW(optimizer_parameters, lr=self.hparams.lr)\n","\n","        scheduler = get_linear_schedule_with_warmup(\n","            optimizer,\n","            num_warmup_steps=0,\n","            num_training_steps=self.total_steps(),\n","        )\n","\n","        return [optimizer],  [{\"scheduler\": scheduler, \"interval\": \"step\"}]\n","\n","    def train_dataloader(self):\n","        vqa_dataset=VQA(root='/kaggle/input', answer_to_index=self.ans_to_index,split= 'train', tokenizer=self.tokenizer, max_len=15 )\n","\n","        loader = DataLoader(vqa_dataset, batch_size = self.hparams.batch_size,num_workers=8, shuffle= True)\n","        return loader\n"],"execution_count":5,"outputs":[]},{"metadata":{"id":"7SoSX5_T7AQ1","outputId":"97ee6c59-238e-4f22-b95d-c1aa63619384","trusted":true,"collapsed":true},"cell_type":"code","source":["!wget --header=\"Host: s3.amazonaws.com\" --header=\"User-Agent: Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/81.0.4044.138 Safari/537.36\" --header=\"Accept: text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.9\" --header=\"Accept-Language: en-GB,en-US;q=0.9,en;q=0.8\" \"https://s3.amazonaws.com/cvmlp/vqa/mscoco/vqa/v2_Annotations_Train_mscoco.zip\" -c -O 'v2_Annotations_Train_mscoco.zip'\n","!unzip v2_Annotations_Train_mscoco.zip\n","!wget --header=\"Host: images.cocodataset.org\" --header=\"User-Agent: Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/81.0.4044.138 Safari/537.36\" --header=\"Accept: text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.9\" --header=\"Accept-Language: en-GB,en-US;q=0.9,en;q=0.8\" \"http://images.cocodataset.org/zips/train2014.zip\" -c -O 'train2014.zip'\n","!unzip train2014.zip\n","!wget --header=\"Host: s3.amazonaws.com\" --header=\"User-Agent: Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/81.0.4044.138 Safari/537.36\" --header=\"Accept: text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.9\" --header=\"Accept-Language: en-GB,en-US;q=0.9,en;q=0.8\" \"https://s3.amazonaws.com/cvmlp/vqa/mscoco/vqa/v2_Questions_Train_mscoco.zip\" -c -O 'v2_Questions_Train_mscoco.zip'\n","!unzip v2_Questions_Train_mscoco.zip\n","#!wget --header=\"Host: images.cocodataset.org\" --header=\"User-Agent: Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/81.0.4044.138 Safari/537.36\" --header=\"Accept: text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.9\" --header=\"Accept-Language: en-GB,en-US;q=0.9,en;q=0.8\" \"http://images.cocodataset.org/zips/val2014.zip\" -c -O 'val2014.zip'\n","#!unzip val2014.zip\n","!wget --header=\"Host: s3.amazonaws.com\" --header=\"User-Agent: Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/81.0.4044.138 Safari/537.36\" --header=\"Accept: text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.9\" --header=\"Accept-Language: en-GB,en-US;q=0.9,en;q=0.8\" \"https://s3.amazonaws.com/cvmlp/vqa/mscoco/vqa/v2_Questions_Val_mscoco.zip\" -c -O 'v2_Questions_Val_mscoco.zip'\n","!unzip v2_Questions_Val_mscoco.zip\n","!wget --header=\"Host: s3.amazonaws.com\" --header=\"User-Agent: Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/81.0.4044.138 Safari/537.36\" --header=\"Accept: text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.9\" --header=\"Accept-Language: en-GB,en-US;q=0.9,en;q=0.8\" \"https://s3.amazonaws.com/cvmlp/vqa/mscoco/vqa/v2_Questions_Test_mscoco.zip\" -c -O 'v2_Questions_Test_mscoco.zip'\n","#!unzip v2_Questions_Test_mscoco.zip\n","#!wget --header=\"Host: images.cocodataset.org\" --header=\"User-Agent: Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/81.0.4044.138 Safari/537.36\" --header=\"Accept: text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.9\" --header=\"Accept-Language: en-GB,en-US;q=0.9,en;q=0.8\" \"http://images.cocodataset.org/zips/test2015.zip\" -c -O 'test2015.zip'\n","#!unzip test2015.zip"],"execution_count":null,"outputs":[]},{"metadata":{"id":"NiPadTPQbG0u","trusted":true},"cell_type":"code","source":["\n","def assert_eq(real, expected):\n","    assert real == expected, \"%s (true) vs %s (expected)\" % (real, expected)\n","\n","def _create_entry(question, answer):\n","    answer.pop(\"image_id\")\n","    answer.pop(\"question_id\")\n","    entry = {\n","        \"question_id\": question[\"question_id\"],\n","        \"image_id\": question[\"image_id\"],\n","        \"question\": question[\"question\"],\n","        \"answer\": [a['answer'] for a in answer['answers']],\n","    }\n","    return entry\n","\n","def _load_dataset(dataroot, name):\n","    \"\"\"Load entries\n","    dataroot: root path of dataset\n","    name: 'train', 'val', 'trainval', 'minsval'\n","    \"\"\"\n","    if name == 'train' or name == 'val':\n","        question_path = os.path.join(dataroot, \"v2_OpenEnded_mscoco_%s2014_questions.json\" % name)\n","        questions = sorted(json.load(open(question_path))[\"questions\"], key=lambda x: x[\"question_id\"])\n","        answer_path = os.path.join(dataroot, \"v2_mscoco_%s2014_annotations.json\" % name)\n","        answers = json.load(open(answer_path, \"rb\"))[\"annotations\"]\n","        answers = sorted(answers, key=lambda x: x[\"question_id\"])\n","\n","    elif name  == 'trainval':\n","        question_path_train = os.path.join(dataroot, \"v2_OpenEnded_mscoco_%s2014_questions.json\" % 'train')\n","        questions_train = sorted(json.load(open(question_path_train))[\"questions\"], key=lambda x: x[\"question_id\"])\n","        answer_path_train = os.path.join(dataroot, \"v2_mscoco_%s2014_annotations.json\" % 'train')\n","        answers_train = json.load(open(answer_path_train, \"rb\"))[\"annotations\"]\n","        answers_train = sorted(answers_train, key=lambda x: x[\"question_id\"])\n","\n","        question_path_val = os.path.join(dataroot, \"v2_OpenEnded_mscoco_%s2014_questions.json\" % 'val')\n","        questions_val = sorted(json.load(open(question_path_val))[\"questions\"], key=lambda x: x[\"question_id\"])\n","        answer_path_val = os.path.join(dataroot, \"v2_mscoco_%s2014_annotations.json\" % 'val')\n","        answers_val = json.load(open(answer_path_val, \"rb\"))[\"annotations\"]\n","        answers_val = sorted(answers_val, key=lambda x: x[\"question_id\"])\n","        questions = questions_train + questions_val[:-3000]\n","        answers = answers_train + answers_val[:-3000]\n","\n","    elif name == 'minval':\n","        question_path_val = os.path.join(dataroot, \"v2_OpenEnded_mscoco_%s2014_questions.json\" % 'val')\n","        questions_val = sorted(json.load(open(question_path_val))[\"questions\"], key=lambda x: x[\"question_id\"])\n","        answer_path_val = os.path.join(dataroot, \"v2_mscoco_%s2014_annotations.json\" % 'val')\n","        answers_val = json.load(open(answer_path_val, \"rb\"))[\"annotations\"]\n","        answers_val = sorted(answers_val, key=lambda x: x[\"question_id\"])        \n","        questions = questions_val[-3000:]\n","        answers = answers_val[-3000:]\n","\n","    elif name == 'test':\n","        question_path_test = os.path.join(dataroot, \"v2_OpenEnded_mscoco_%s2015_questions.json\" % 'test')\n","        questions_test = sorted(json.load(open(question_path_test))[\"questions\"], key=lambda x: x[\"question_id\"])\n","        questions = questions_test\n","    else:\n","        assert False, \"data split is not recognized.\"\n","\n","    if 'test' in name:\n","        entries = []\n","        for question in questions:\n","            entries.append(question)\n","    else:\n","        assert_eq(len(questions), len(answers))\n","        entries = []\n","        for question, answer in zip(questions, answers):\n","            assert_eq(question[\"question_id\"], answer[\"question_id\"])\n","            assert_eq(question[\"image_id\"], answer[\"image_id\"])\n","            entries.append(_create_entry(question, answer))\n","    return entries"],"execution_count":6,"outputs":[]},{"metadata":{"id":"c_KSLg6prP4J","trusted":true},"cell_type":"code","source":["entries = _load_dataset(dataroot='',name='train')\n"],"execution_count":7,"outputs":[]},{"metadata":{"id":"tdewFhEzUU1d","trusted":true},"cell_type":"code","source":["# compile a list of all the answers\n","all_answers  = set()\n","for a in entries:\n","    all_answers.update(a['answer'])\n","all_answers=list(all_answers)\n"],"execution_count":8,"outputs":[]},{"metadata":{"id":"d-Q_quxQmxOv","trusted":true},"cell_type":"code","source":["answer_to_index = dict()\n","for i,answer in enumerate(all_answers):\n","    answer_to_index[answer]=i\n"],"execution_count":9,"outputs":[]},{"metadata":{"id":"BBlfQ2YngeTd","trusted":true},"cell_type":"code","source":["class VQA(data.Dataset):\n","    \"\"\" VQA dataset, open-ended \"\"\"\n","    def __init__(self, root, answer_to_index, tokenizer ,split = 'train', max_len = 20):\n","        super(VQA, self).__init__()\n","\n","\n","        self.root = root\n","        self.answer_to_index = answer_to_index\n","        self.split = split\n","        self.tokenizer = tokenizer\n","        self.max_len = max_len\n","        self.entries = self._load_dataset( self.root, self.split)\n","\n","         # standard PyTorch mean-std input image normalization\n","        self.transform = T.Compose([\n","            T.Resize(size=(800,800)),\n","            T.ToTensor(),\n","            T.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n","        ])\n","\n","        self.id_to_image_fname = self._find_iamges()\n","\n","\n","    def assert_eq(self,real, expected):\n","        assert real == expected, \"%s (true) vs %s (expected)\" % (real, expected)\n","\n","    def _create_entry(self,question, answer):\n","        answer.pop(\"image_id\")\n","        answer.pop(\"question_id\")\n","        entry = {\n","            \"question_id\": question[\"question_id\"],\n","            \"image_id\": question[\"image_id\"],\n","            \"question\": question[\"question\"],\n","            \"answer\": [a['answer'] for a in answer['answers']],\n","        }\n","        return entry\n","\n","    def _load_dataset(self,dataroot, name):\n","        \"\"\"Load entries\n","        dataroot: root path of dataset\n","        name: 'train', 'val', 'trainval', 'minsval'\n","        \"\"\"\n","        #dataroot +=\"/vqa-data/VQA_Data/\"\n","        if name == 'train' or name == 'val':\n","            question_path = os.path.join(dataroot, \"v2_OpenEnded_mscoco_%s2014_questions.json\" % name)\n","            questions = sorted(json.load(open(question_path))[\"questions\"], key=lambda x: x[\"question_id\"])\n","            answer_path = os.path.join(dataroot, \"v2_mscoco_%s2014_annotations.json\" % name)\n","            answers = json.load(open(answer_path, \"rb\"))[\"annotations\"]\n","            answers = sorted(answers, key=lambda x: x[\"question_id\"])\n","\n","        elif name  == 'trainval':\n","            question_path_train = os.path.join(dataroot, \"v2_OpenEnded_mscoco_%s2014_questions.json\" % 'train')\n","            questions_train = sorted(json.load(open(question_path_train))[\"questions\"], key=lambda x: x[\"question_id\"])\n","            answer_path_train = os.path.join(dataroot, \"v2_mscoco_%s2014_annotations.json\" % 'train')\n","            answers_train = json.load(open(answer_path_train, \"rb\"))[\"annotations\"]\n","            answers_train = sorted(answers_train, key=lambda x: x[\"question_id\"])\n","\n","            question_path_val = os.path.join(dataroot, \"v2_OpenEnded_mscoco_%s2014_questions.json\" % 'val')\n","            questions_val = sorted(json.load(open(question_path_val))[\"questions\"], key=lambda x: x[\"question_id\"])\n","            answer_path_val = os.path.join(dataroot, \"v2_mscoco_%s2014_annotations.json\" % 'val')\n","            answers_val = json.load(open(answer_path_val, \"rb\"))[\"annotations\"]\n","            answers_val = sorted(answers_val, key=lambda x: x[\"question_id\"])\n","            questions = questions_train + questions_val[:-3000]\n","            answers = answers_train + answers_val[:-3000]\n","\n","        elif name == 'minval':\n","            question_path_val = os.path.join(dataroot, \"v2_OpenEnded_mscoco_%s2014_questions.json\" % 'val')\n","            questions_val = sorted(json.load(open(question_path_val))[\"questions\"], key=lambda x: x[\"question_id\"])\n","            answer_path_val = os.path.join(dataroot, \"v2_mscoco_%s2014_annotations.json\" % 'val')\n","            answers_val = json.load(open(answer_path_val, \"rb\"))[\"annotations\"]\n","            answers_val = sorted(answers_val, key=lambda x: x[\"question_id\"])        \n","            questions = questions_val[-3000:]\n","            answers = answers_val[-3000:]\n","\n","        elif name == 'test':\n","            question_path_test = os.path.join(dataroot, \"v2_OpenEnded_mscoco_%s2015_questions.json\" % 'test')\n","            questions_test = sorted(json.load(open(question_path_test))[\"questions\"], key=lambda x: x[\"question_id\"])\n","            questions = questions_test\n","        else:\n","            assert False, \"data split is not recognized.\"\n","\n","        if 'test' in name:\n","            entries = []\n","            for question in questions:\n","                entries.append(question)\n","        else:\n","            assert_eq(len(questions), len(answers))\n","            entries = []\n","            for question, answer in zip(questions, answers):\n","                assert_eq(question[\"question_id\"], answer[\"question_id\"])\n","                assert_eq(question[\"image_id\"], answer[\"image_id\"])\n","                entries.append(_create_entry(question, answer))\n","        return entries\n","\n","\n","\n","    def _encode_question(self, question):\n","        \"\"\" Turn a question into a vector of indices and a question length \"\"\"\n","        \n","        inputs = self.tokenizer.encode_plus(\n","            question,\n","            None,\n","            add_special_tokens=True,\n","            max_length=self.max_len,\n","            )\n","\n","        ids = inputs[\"input_ids\"]\n","        mask = inputs[\"attention_mask\"]\n","        token_type_ids = inputs[\"token_type_ids\"]\n","\n","        padding_length = self.max_len - len(ids)\n","        ids += ([0]*padding_length)\n","        mask += ([0]*padding_length)\n","        token_type_ids += ([0]*padding_length)\n","        \n","        return {\n","            'ids': torch.tensor(ids, dtype=torch.long),\n","            'mask': torch.tensor(mask, dtype=torch.long),\n","            'token_type_ids': torch.tensor(token_type_ids, dtype=torch.long),\n","            \n","        }\n","\n","    def _encode_ansfor_bert(self, answers):\n","        pass\n","\n","\n","    def _encode_answers(self, answers):\n","        \"\"\" Turn an answer into a vector \"\"\"\n","        # answer vec will be a vector of answer counts to determine which answers will contribute to the loss.\n","        # this should be multiplied with 0.1 * negative log-likelihoods that a model produces and then summed up\n","        # to get the loss that is weighted by how many humans gave that answer\n","        answer_vec = torch.zeros(len(self.answer_to_index),dtype=torch.long)\n","        for answer in answers:\n","            index = self.answer_to_index.get(answer)\n","            if index is not None:\n","                answer_vec[index] += 1\n","        _,idx = answer_vec.max(dim = 0)\n","        idx = torch.tensor(idx, dtype = torch.long)\n","\n","        return idx\n","\n","   \n","    def _find_iamges(self):\n","        id_to_filename = {}\n","        imgs_folder = os.path.join(self.root,'coco2014/train2014/train2014')\n","        for filename in os.listdir(imgs_folder):\n","            if not filename.endswith('.jpg'):\n","                continue\n","            id_and_extension = filename.split('_')[-1]\n","            id = int(id_and_extension.split('.')[0])\n","            id_to_filename[id] = os.path.join(imgs_folder,filename)\n","        return id_to_filename\n","\n","\n","    def _load_image(self, image_id):\n","        \"\"\" Load an image \"\"\"\n","\n","        img_path = self.id_to_image_fname[image_id]\n","        img  = Image.open(img_path)\n","        img = np.asarray(img)\n","        \n","        if len(img.shape)==2:\n","            print(img.shape)\n","            img=np.expand_dims(img, axis=-1)\n","            \n","            img = np.repeat(img,3, axis = -1)\n","            print(img.shape)\n","\n","        return img\n","\n","    def __getitem__(self, item):\n","       \n","        entry  = self.entries[item]\n","        q = entry['question']\n","        a = self._encode_answers(entry['answer'])\n","        image_id = entry['image_id']\n","\n","        img = self._load_image(image_id)\n","        img = Image.fromarray(img)\n","        img = self.transform(img)\n","        #question_id = entry['question_id']\n","        q= self._encode_question(q)\n","\n","        return img, q, a\n","\n","    def __len__(self):\n","        return len(self.entries)\n","\n","    @staticmethod\n","    def collate_fn(batch):\n","        \"\"\"The collat_fn method to be used by the\n","        PyTorch data loader.\n","        \"\"\"\n","        \n","        # Unzip the batch\n","\n","        imgs,qs, answers = list(zip(*batch))\n","\n","        # concatenate the vectors\n","        imgs = torch.stack(imgs)\n","        \n","        #concatenate the labels\n","        q = torch.stack(qs)\n","\n","        a = torch.stack(answers)\n","        \n","        return imgs, q, a"],"execution_count":10,"outputs":[]},{"metadata":{"id":"Ff9kOL6KIncK","outputId":"38d7fc31-2edb-42d7-b889-e2aa1ecd084f","trusted":true},"cell_type":"code","source":["#bert_tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)"],"execution_count":11,"outputs":[]},{"metadata":{"id":"rhYh98-8GROJ","trusted":true},"cell_type":"code","source":["#vqa_data = VQA(root='/kaggle/input', answer_to_index=answer_to_index,split= 'train', tokenizer=bert_tokenizer, max_len=15 )"],"execution_count":12,"outputs":[]},{"metadata":{"id":"UKR94pUILXJm","trusted":true},"cell_type":"code","source":["hparams = Namespace(\n","    batch_size=32,\n","    warmup_steps=100,\n","    epochs=3,\n","    lr=3e-5,\n","    accumulate_grad_batches=1\n",")"],"execution_count":21,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":["len_ans = len(answer_to_index)"],"execution_count":22,"outputs":[]},{"metadata":{"id":"uhbtt7a2OfCk","outputId":"0dbea896-b353-4924-94f8-b4819cd44fb0","trusted":true},"cell_type":"code","source":["vqa_detr = VQA_DETR(num_ans=len_ans,ans_to_index=answer_to_index,hparams=hparams)"],"execution_count":23,"outputs":[]},{"metadata":{"id":"cXeflNl8HEy7","trusted":true},"cell_type":"code","source":["\n","#data_loader = DataLoader(vqa_data, batch_size = hparams.batch_size, shuffle= True)"],"execution_count":16,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":["#del answer_to_index\n","del all_answers\n","del entries"],"execution_count":17,"outputs":[]},{"metadata":{"id":"tosN3pPcRCQZ","outputId":"c756bae6-e107-4448-be34-ec94a81ea547","trusted":true},"cell_type":"code","source":["!free -h"],"execution_count":24,"outputs":[{"output_type":"stream","text":"              total        used        free      shared  buff/cache   available\r\nMem:            15G        5.9G        7.3G         12M        2.4G        9.5G\r\nSwap:            0B          0B          0B\r\n","name":"stdout"}]},{"metadata":{"id":"WV4y-BdvRIZj","outputId":"8c1c7bbb-ab49-4399-85a2-8e055a5e0354","trusted":true},"cell_type":"code","source":["!cat /proc/cpuinfo | grep processor | wc -l"],"execution_count":25,"outputs":[{"output_type":"stream","text":"2\r\n","name":"stdout"}]},{"metadata":{"id":"YGZ-OqgvOe-u","outputId":"a87f41f7-904f-476e-9d5b-bafee564ccdb","trusted":true},"cell_type":"code","source":["trainer = Trainer(gpus=1, train_percent_check=0.2, max_epochs=50)\n","trainer.fit(vqa_detr)"],"execution_count":null,"outputs":[{"output_type":"stream","text":"GPU available: True, used: True\nNo environment variable for node rank defined. Set as 0.\nCUDA_VISIBLE_DEVICES: [0]\n\n    | Name                                                         | Type                    | Params\n-----------------------------------------------------------------------------------------------------\n0   | bert_decoder                                                 | BertModel               | 20 M  \n1   | bert_decoder.embeddings                                      | BertEmbeddings          | 7 M   \n2   | bert_decoder.embeddings.word_embeddings                      | Embedding               | 7 M   \n3   | bert_decoder.embeddings.position_embeddings                  | Embedding               | 131 K \n4   | bert_decoder.embeddings.token_type_embeddings                | Embedding               | 512   \n5   | bert_decoder.embeddings.LayerNorm                            | LayerNorm               | 512   \n6   | bert_decoder.embeddings.dropout                              | Dropout                 | 0     \n7   | bert_decoder.encoder                                         | BertEncoder             | 12 M  \n8   | bert_decoder.encoder.layer                                   | ModuleList              | 12 M  \n9   | bert_decoder.encoder.layer.0                                 | BertLayer               | 2 M   \n10  | bert_decoder.encoder.layer.0.attention                       | BertAttention           | 263 K \n11  | bert_decoder.encoder.layer.0.attention.self                  | BertSelfAttention       | 197 K \n12  | bert_decoder.encoder.layer.0.attention.self.query            | Linear                  | 65 K  \n13  | bert_decoder.encoder.layer.0.attention.self.key              | Linear                  | 65 K  \n14  | bert_decoder.encoder.layer.0.attention.self.value            | Linear                  | 65 K  \n15  | bert_decoder.encoder.layer.0.attention.self.dropout          | Dropout                 | 0     \n16  | bert_decoder.encoder.layer.0.attention.output                | BertSelfOutput          | 66 K  \n17  | bert_decoder.encoder.layer.0.attention.output.dense          | Linear                  | 65 K  \n18  | bert_decoder.encoder.layer.0.attention.output.LayerNorm      | LayerNorm               | 512   \n19  | bert_decoder.encoder.layer.0.attention.output.dropout        | Dropout                 | 0     \n20  | bert_decoder.encoder.layer.0.crossattention                  | BertAttention           | 263 K \n21  | bert_decoder.encoder.layer.0.crossattention.self             | BertSelfAttention       | 197 K \n22  | bert_decoder.encoder.layer.0.crossattention.self.query       | Linear                  | 65 K  \n23  | bert_decoder.encoder.layer.0.crossattention.self.key         | Linear                  | 65 K  \n24  | bert_decoder.encoder.layer.0.crossattention.self.value       | Linear                  | 65 K  \n25  | bert_decoder.encoder.layer.0.crossattention.self.dropout     | Dropout                 | 0     \n26  | bert_decoder.encoder.layer.0.crossattention.output           | BertSelfOutput          | 66 K  \n27  | bert_decoder.encoder.layer.0.crossattention.output.dense     | Linear                  | 65 K  \n28  | bert_decoder.encoder.layer.0.crossattention.output.LayerNorm | LayerNorm               | 512   \n29  | bert_decoder.encoder.layer.0.crossattention.output.dropout   | Dropout                 | 0     \n30  | bert_decoder.encoder.layer.0.intermediate                    | BertIntermediate        | 789 K \n31  | bert_decoder.encoder.layer.0.intermediate.dense              | Linear                  | 789 K \n32  | bert_decoder.encoder.layer.0.output                          | BertOutput              | 787 K \n33  | bert_decoder.encoder.layer.0.output.dense                    | Linear                  | 786 K \n34  | bert_decoder.encoder.layer.0.output.LayerNorm                | LayerNorm               | 512   \n35  | bert_decoder.encoder.layer.0.output.dropout                  | Dropout                 | 0     \n36  | bert_decoder.encoder.layer.1                                 | BertLayer               | 2 M   \n37  | bert_decoder.encoder.layer.1.attention                       | BertAttention           | 263 K \n38  | bert_decoder.encoder.layer.1.attention.self                  | BertSelfAttention       | 197 K \n39  | bert_decoder.encoder.layer.1.attention.self.query            | Linear                  | 65 K  \n40  | bert_decoder.encoder.layer.1.attention.self.key              | Linear                  | 65 K  \n41  | bert_decoder.encoder.layer.1.attention.self.value            | Linear                  | 65 K  \n42  | bert_decoder.encoder.layer.1.attention.self.dropout          | Dropout                 | 0     \n43  | bert_decoder.encoder.layer.1.attention.output                | BertSelfOutput          | 66 K  \n44  | bert_decoder.encoder.layer.1.attention.output.dense          | Linear                  | 65 K  \n45  | bert_decoder.encoder.layer.1.attention.output.LayerNorm      | LayerNorm               | 512   \n46  | bert_decoder.encoder.layer.1.attention.output.dropout        | Dropout                 | 0     \n47  | bert_decoder.encoder.layer.1.crossattention                  | BertAttention           | 263 K \n48  | bert_decoder.encoder.layer.1.crossattention.self             | BertSelfAttention       | 197 K \n49  | bert_decoder.encoder.layer.1.crossattention.self.query       | Linear                  | 65 K  \n50  | bert_decoder.encoder.layer.1.crossattention.self.key         | Linear                  | 65 K  \n51  | bert_decoder.encoder.layer.1.crossattention.self.value       | Linear                  | 65 K  \n52  | bert_decoder.encoder.layer.1.crossattention.self.dropout     | Dropout                 | 0     \n53  | bert_decoder.encoder.layer.1.crossattention.output           | BertSelfOutput          | 66 K  \n54  | bert_decoder.encoder.layer.1.crossattention.output.dense     | Linear                  | 65 K  \n55  | bert_decoder.encoder.layer.1.crossattention.output.LayerNorm | LayerNorm               | 512   \n56  | bert_decoder.encoder.layer.1.crossattention.output.dropout   | Dropout                 | 0     \n57  | bert_decoder.encoder.layer.1.intermediate                    | BertIntermediate        | 789 K \n58  | bert_decoder.encoder.layer.1.intermediate.dense              | Linear                  | 789 K \n59  | bert_decoder.encoder.layer.1.output                          | BertOutput              | 787 K \n60  | bert_decoder.encoder.layer.1.output.dense                    | Linear                  | 786 K \n61  | bert_decoder.encoder.layer.1.output.LayerNorm                | LayerNorm               | 512   \n62  | bert_decoder.encoder.layer.1.output.dropout                  | Dropout                 | 0     \n63  | bert_decoder.encoder.layer.2                                 | BertLayer               | 2 M   \n64  | bert_decoder.encoder.layer.2.attention                       | BertAttention           | 263 K \n65  | bert_decoder.encoder.layer.2.attention.self                  | BertSelfAttention       | 197 K \n66  | bert_decoder.encoder.layer.2.attention.self.query            | Linear                  | 65 K  \n67  | bert_decoder.encoder.layer.2.attention.self.key              | Linear                  | 65 K  \n68  | bert_decoder.encoder.layer.2.attention.self.value            | Linear                  | 65 K  \n69  | bert_decoder.encoder.layer.2.attention.self.dropout          | Dropout                 | 0     \n70  | bert_decoder.encoder.layer.2.attention.output                | BertSelfOutput          | 66 K  \n71  | bert_decoder.encoder.layer.2.attention.output.dense          | Linear                  | 65 K  \n72  | bert_decoder.encoder.layer.2.attention.output.LayerNorm      | LayerNorm               | 512   \n73  | bert_decoder.encoder.layer.2.attention.output.dropout        | Dropout                 | 0     \n74  | bert_decoder.encoder.layer.2.crossattention                  | BertAttention           | 263 K \n75  | bert_decoder.encoder.layer.2.crossattention.self             | BertSelfAttention       | 197 K \n76  | bert_decoder.encoder.layer.2.crossattention.self.query       | Linear                  | 65 K  \n77  | bert_decoder.encoder.layer.2.crossattention.self.key         | Linear                  | 65 K  \n78  | bert_decoder.encoder.layer.2.crossattention.self.value       | Linear                  | 65 K  \n79  | bert_decoder.encoder.layer.2.crossattention.self.dropout     | Dropout                 | 0     \n80  | bert_decoder.encoder.layer.2.crossattention.output           | BertSelfOutput          | 66 K  \n81  | bert_decoder.encoder.layer.2.crossattention.output.dense     | Linear                  | 65 K  \n82  | bert_decoder.encoder.layer.2.crossattention.output.LayerNorm | LayerNorm               | 512   \n83  | bert_decoder.encoder.layer.2.crossattention.output.dropout   | Dropout                 | 0     \n84  | bert_decoder.encoder.layer.2.intermediate                    | BertIntermediate        | 789 K \n85  | bert_decoder.encoder.layer.2.intermediate.dense              | Linear                  | 789 K \n86  | bert_decoder.encoder.layer.2.output                          | BertOutput              | 787 K \n87  | bert_decoder.encoder.layer.2.output.dense                    | Linear                  | 786 K \n88  | bert_decoder.encoder.layer.2.output.LayerNorm                | LayerNorm               | 512   \n89  | bert_decoder.encoder.layer.2.output.dropout                  | Dropout                 | 0     \n90  | bert_decoder.encoder.layer.3                                 | BertLayer               | 2 M   \n91  | bert_decoder.encoder.layer.3.attention                       | BertAttention           | 263 K \n92  | bert_decoder.encoder.layer.3.attention.self                  | BertSelfAttention       | 197 K \n93  | bert_decoder.encoder.layer.3.attention.self.query            | Linear                  | 65 K  \n94  | bert_decoder.encoder.layer.3.attention.self.key              | Linear                  | 65 K  \n95  | bert_decoder.encoder.layer.3.attention.self.value            | Linear                  | 65 K  \n96  | bert_decoder.encoder.layer.3.attention.self.dropout          | Dropout                 | 0     \n97  | bert_decoder.encoder.layer.3.attention.output                | BertSelfOutput          | 66 K  \n98  | bert_decoder.encoder.layer.3.attention.output.dense          | Linear                  | 65 K  \n99  | bert_decoder.encoder.layer.3.attention.output.LayerNorm      | LayerNorm               | 512   \n100 | bert_decoder.encoder.layer.3.attention.output.dropout        | Dropout                 | 0     \n101 | bert_decoder.encoder.layer.3.crossattention                  | BertAttention           | 263 K \n102 | bert_decoder.encoder.layer.3.crossattention.self             | BertSelfAttention       | 197 K \n103 | bert_decoder.encoder.layer.3.crossattention.self.query       | Linear                  | 65 K  \n104 | bert_decoder.encoder.layer.3.crossattention.self.key         | Linear                  | 65 K  \n105 | bert_decoder.encoder.layer.3.crossattention.self.value       | Linear                  | 65 K  \n106 | bert_decoder.encoder.layer.3.crossattention.self.dropout     | Dropout                 | 0     \n107 | bert_decoder.encoder.layer.3.crossattention.output           | BertSelfOutput          | 66 K  \n108 | bert_decoder.encoder.layer.3.crossattention.output.dense     | Linear                  | 65 K  \n109 | bert_decoder.encoder.layer.3.crossattention.output.LayerNorm | LayerNorm               | 512   \n110 | bert_decoder.encoder.layer.3.crossattention.output.dropout   | Dropout                 | 0     \n111 | bert_decoder.encoder.layer.3.intermediate                    | BertIntermediate        | 789 K \n112 | bert_decoder.encoder.layer.3.intermediate.dense              | Linear                  | 789 K \n113 | bert_decoder.encoder.layer.3.output                          | BertOutput              | 787 K \n114 | bert_decoder.encoder.layer.3.output.dense                    | Linear                  | 786 K \n115 | bert_decoder.encoder.layer.3.output.LayerNorm                | LayerNorm               | 512   \n116 | bert_decoder.encoder.layer.3.output.dropout                  | Dropout                 | 0     \n117 | bert_decoder.encoder.layer.4                                 | BertLayer               | 2 M   \n118 | bert_decoder.encoder.layer.4.attention                       | BertAttention           | 263 K \n119 | bert_decoder.encoder.layer.4.attention.self                  | BertSelfAttention       | 197 K \n120 | bert_decoder.encoder.layer.4.attention.self.query            | Linear                  | 65 K  \n121 | bert_decoder.encoder.layer.4.attention.self.key              | Linear                  | 65 K  \n122 | bert_decoder.encoder.layer.4.attention.self.value            | Linear                  | 65 K  \n123 | bert_decoder.encoder.layer.4.attention.self.dropout          | Dropout                 | 0     \n124 | bert_decoder.encoder.layer.4.attention.output                | BertSelfOutput          | 66 K  \n125 | bert_decoder.encoder.layer.4.attention.output.dense          | Linear                  | 65 K  \n126 | bert_decoder.encoder.layer.4.attention.output.LayerNorm      | LayerNorm               | 512   \n127 | bert_decoder.encoder.layer.4.attention.output.dropout        | Dropout                 | 0     \n128 | bert_decoder.encoder.layer.4.crossattention                  | BertAttention           | 263 K \n129 | bert_decoder.encoder.layer.4.crossattention.self             | BertSelfAttention       | 197 K \n130 | bert_decoder.encoder.layer.4.crossattention.self.query       | Linear                  | 65 K  \n131 | bert_decoder.encoder.layer.4.crossattention.self.key         | Linear                  | 65 K  \n132 | bert_decoder.encoder.layer.4.crossattention.self.value       | Linear                  | 65 K  \n133 | bert_decoder.encoder.layer.4.crossattention.self.dropout     | Dropout                 | 0     \n134 | bert_decoder.encoder.layer.4.crossattention.output           | BertSelfOutput          | 66 K  \n135 | bert_decoder.encoder.layer.4.crossattention.output.dense     | Linear                  | 65 K  \n136 | bert_decoder.encoder.layer.4.crossattention.output.LayerNorm | LayerNorm               | 512   \n137 | bert_decoder.encoder.layer.4.crossattention.output.dropout   | Dropout                 | 0     \n138 | bert_decoder.encoder.layer.4.intermediate                    | BertIntermediate        | 789 K \n139 | bert_decoder.encoder.layer.4.intermediate.dense              | Linear                  | 789 K \n140 | bert_decoder.encoder.layer.4.output                          | BertOutput              | 787 K \n141 | bert_decoder.encoder.layer.4.output.dense                    | Linear                  | 786 K \n142 | bert_decoder.encoder.layer.4.output.LayerNorm                | LayerNorm               | 512   \n143 | bert_decoder.encoder.layer.4.output.dropout                  | Dropout                 | 0     \n144 | bert_decoder.encoder.layer.5                                 | BertLayer               | 2 M   \n145 | bert_decoder.encoder.layer.5.attention                       | BertAttention           | 263 K \n146 | bert_decoder.encoder.layer.5.attention.self                  | BertSelfAttention       | 197 K \n147 | bert_decoder.encoder.layer.5.attention.self.query            | Linear                  | 65 K  \n148 | bert_decoder.encoder.layer.5.attention.self.key              | Linear                  | 65 K  \n149 | bert_decoder.encoder.layer.5.attention.self.value            | Linear                  | 65 K  \n150 | bert_decoder.encoder.layer.5.attention.self.dropout          | Dropout                 | 0     \n151 | bert_decoder.encoder.layer.5.attention.output                | BertSelfOutput          | 66 K  \n152 | bert_decoder.encoder.layer.5.attention.output.dense          | Linear                  | 65 K  \n153 | bert_decoder.encoder.layer.5.attention.output.LayerNorm      | LayerNorm               | 512   \n154 | bert_decoder.encoder.layer.5.attention.output.dropout        | Dropout                 | 0     \n155 | bert_decoder.encoder.layer.5.crossattention                  | BertAttention           | 263 K \n156 | bert_decoder.encoder.layer.5.crossattention.self             | BertSelfAttention       | 197 K \n157 | bert_decoder.encoder.layer.5.crossattention.self.query       | Linear                  | 65 K  \n158 | bert_decoder.encoder.layer.5.crossattention.self.key         | Linear                  | 65 K  \n159 | bert_decoder.encoder.layer.5.crossattention.self.value       | Linear                  | 65 K  \n160 | bert_decoder.encoder.layer.5.crossattention.self.dropout     | Dropout                 | 0     \n161 | bert_decoder.encoder.layer.5.crossattention.output           | BertSelfOutput          | 66 K  \n162 | bert_decoder.encoder.layer.5.crossattention.output.dense     | Linear                  | 65 K  \n163 | bert_decoder.encoder.layer.5.crossattention.output.LayerNorm | LayerNorm               | 512   \n164 | bert_decoder.encoder.layer.5.crossattention.output.dropout   | Dropout                 | 0     \n165 | bert_decoder.encoder.layer.5.intermediate                    | BertIntermediate        | 789 K \n166 | bert_decoder.encoder.layer.5.intermediate.dense              | Linear                  | 789 K \n167 | bert_decoder.encoder.layer.5.output                          | BertOutput              | 787 K \n168 | bert_decoder.encoder.layer.5.output.dense                    | Linear                  | 786 K \n169 | bert_decoder.encoder.layer.5.output.LayerNorm                | LayerNorm               | 512   \n170 | bert_decoder.encoder.layer.5.output.dropout                  | Dropout                 | 0     \n171 | bert_decoder.pooler                                          | BertPooler              | 65 K  \n172 | bert_decoder.pooler.dense                                    | Linear                  | 65 K  \n173 | bert_decoder.pooler.activation                               | Tanh                    | 0     \n174 | detr                                                         | DETRdemo                | 41 M  \n175 | detr.backbone                                                | ResNet                  | 23 M  \n176 | detr.backbone.conv1                                          | Conv2d                  | 9 K   \n177 | detr.backbone.bn1                                            | BatchNorm2d             | 128   \n178 | detr.backbone.relu                                           | ReLU                    | 0     \n179 | detr.backbone.maxpool                                        | MaxPool2d               | 0     \n180 | detr.backbone.layer1                                         | Sequential              | 215 K \n181 | detr.backbone.layer1.0                                       | Bottleneck              | 75 K  \n182 | detr.backbone.layer1.0.conv1                                 | Conv2d                  | 4 K   \n183 | detr.backbone.layer1.0.bn1                                   | BatchNorm2d             | 128   \n184 | detr.backbone.layer1.0.conv2                                 | Conv2d                  | 36 K  \n185 | detr.backbone.layer1.0.bn2                                   | BatchNorm2d             | 128   \n186 | detr.backbone.layer1.0.conv3                                 | Conv2d                  | 16 K  \n187 | detr.backbone.layer1.0.bn3                                   | BatchNorm2d             | 512   \n188 | detr.backbone.layer1.0.relu                                  | ReLU                    | 0     \n189 | detr.backbone.layer1.0.downsample                            | Sequential              | 16 K  \n190 | detr.backbone.layer1.0.downsample.0                          | Conv2d                  | 16 K  \n191 | detr.backbone.layer1.0.downsample.1                          | BatchNorm2d             | 512   \n192 | detr.backbone.layer1.1                                       | Bottleneck              | 70 K  \n193 | detr.backbone.layer1.1.conv1                                 | Conv2d                  | 16 K  \n194 | detr.backbone.layer1.1.bn1                                   | BatchNorm2d             | 128   \n195 | detr.backbone.layer1.1.conv2                                 | Conv2d                  | 36 K  \n196 | detr.backbone.layer1.1.bn2                                   | BatchNorm2d             | 128   \n197 | detr.backbone.layer1.1.conv3                                 | Conv2d                  | 16 K  \n198 | detr.backbone.layer1.1.bn3                                   | BatchNorm2d             | 512   \n199 | detr.backbone.layer1.1.relu                                  | ReLU                    | 0     \n200 | detr.backbone.layer1.2                                       | Bottleneck              | 70 K  \n201 | detr.backbone.layer1.2.conv1                                 | Conv2d                  | 16 K  \n202 | detr.backbone.layer1.2.bn1                                   | BatchNorm2d             | 128   \n203 | detr.backbone.layer1.2.conv2                                 | Conv2d                  | 36 K  \n204 | detr.backbone.layer1.2.bn2                                   | BatchNorm2d             | 128   \n205 | detr.backbone.layer1.2.conv3                                 | Conv2d                  | 16 K  \n206 | detr.backbone.layer1.2.bn3                                   | BatchNorm2d             | 512   \n207 | detr.backbone.layer1.2.relu                                  | ReLU                    | 0     \n208 | detr.backbone.layer2                                         | Sequential              | 1 M   \n209 | detr.backbone.layer2.0                                       | Bottleneck              | 379 K \n210 | detr.backbone.layer2.0.conv1                                 | Conv2d                  | 32 K  \n211 | detr.backbone.layer2.0.bn1                                   | BatchNorm2d             | 256   \n212 | detr.backbone.layer2.0.conv2                                 | Conv2d                  | 147 K \n213 | detr.backbone.layer2.0.bn2                                   | BatchNorm2d             | 256   \n214 | detr.backbone.layer2.0.conv3                                 | Conv2d                  | 65 K  \n215 | detr.backbone.layer2.0.bn3                                   | BatchNorm2d             | 1 K   \n216 | detr.backbone.layer2.0.relu                                  | ReLU                    | 0     \n217 | detr.backbone.layer2.0.downsample                            | Sequential              | 132 K \n218 | detr.backbone.layer2.0.downsample.0                          | Conv2d                  | 131 K \n219 | detr.backbone.layer2.0.downsample.1                          | BatchNorm2d             | 1 K   \n220 | detr.backbone.layer2.1                                       | Bottleneck              | 280 K \n221 | detr.backbone.layer2.1.conv1                                 | Conv2d                  | 65 K  \n222 | detr.backbone.layer2.1.bn1                                   | BatchNorm2d             | 256   \n223 | detr.backbone.layer2.1.conv2                                 | Conv2d                  | 147 K \n224 | detr.backbone.layer2.1.bn2                                   | BatchNorm2d             | 256   \n225 | detr.backbone.layer2.1.conv3                                 | Conv2d                  | 65 K  \n226 | detr.backbone.layer2.1.bn3                                   | BatchNorm2d             | 1 K   \n227 | detr.backbone.layer2.1.relu                                  | ReLU                    | 0     \n228 | detr.backbone.layer2.2                                       | Bottleneck              | 280 K \n229 | detr.backbone.layer2.2.conv1                                 | Conv2d                  | 65 K  \n230 | detr.backbone.layer2.2.bn1                                   | BatchNorm2d             | 256   \n231 | detr.backbone.layer2.2.conv2                                 | Conv2d                  | 147 K \n232 | detr.backbone.layer2.2.bn2                                   | BatchNorm2d             | 256   \n233 | detr.backbone.layer2.2.conv3                                 | Conv2d                  | 65 K  \n234 | detr.backbone.layer2.2.bn3                                   | BatchNorm2d             | 1 K   \n235 | detr.backbone.layer2.2.relu                                  | ReLU                    | 0     \n236 | detr.backbone.layer2.3                                       | Bottleneck              | 280 K \n237 | detr.backbone.layer2.3.conv1                                 | Conv2d                  | 65 K  \n238 | detr.backbone.layer2.3.bn1                                   | BatchNorm2d             | 256   \n239 | detr.backbone.layer2.3.conv2                                 | Conv2d                  | 147 K \n240 | detr.backbone.layer2.3.bn2                                   | BatchNorm2d             | 256   \n241 | detr.backbone.layer2.3.conv3                                 | Conv2d                  | 65 K  \n242 | detr.backbone.layer2.3.bn3                                   | BatchNorm2d             | 1 K   \n243 | detr.backbone.layer2.3.relu                                  | ReLU                    | 0     \n244 | detr.backbone.layer3                                         | Sequential              | 7 M   \n245 | detr.backbone.layer3.0                                       | Bottleneck              | 1 M   \n246 | detr.backbone.layer3.0.conv1                                 | Conv2d                  | 131 K \n247 | detr.backbone.layer3.0.bn1                                   | BatchNorm2d             | 512   \n248 | detr.backbone.layer3.0.conv2                                 | Conv2d                  | 589 K \n249 | detr.backbone.layer3.0.bn2                                   | BatchNorm2d             | 512   \n250 | detr.backbone.layer3.0.conv3                                 | Conv2d                  | 262 K \n251 | detr.backbone.layer3.0.bn3                                   | BatchNorm2d             | 2 K   \n252 | detr.backbone.layer3.0.relu                                  | ReLU                    | 0     \n253 | detr.backbone.layer3.0.downsample                            | Sequential              | 526 K \n254 | detr.backbone.layer3.0.downsample.0                          | Conv2d                  | 524 K \n255 | detr.backbone.layer3.0.downsample.1                          | BatchNorm2d             | 2 K   \n256 | detr.backbone.layer3.1                                       | Bottleneck              | 1 M   \n257 | detr.backbone.layer3.1.conv1                                 | Conv2d                  | 262 K \n258 | detr.backbone.layer3.1.bn1                                   | BatchNorm2d             | 512   \n259 | detr.backbone.layer3.1.conv2                                 | Conv2d                  | 589 K \n260 | detr.backbone.layer3.1.bn2                                   | BatchNorm2d             | 512   \n261 | detr.backbone.layer3.1.conv3                                 | Conv2d                  | 262 K \n262 | detr.backbone.layer3.1.bn3                                   | BatchNorm2d             | 2 K   \n263 | detr.backbone.layer3.1.relu                                  | ReLU                    | 0     \n264 | detr.backbone.layer3.2                                       | Bottleneck              | 1 M   \n265 | detr.backbone.layer3.2.conv1                                 | Conv2d                  | 262 K \n266 | detr.backbone.layer3.2.bn1                                   | BatchNorm2d             | 512   \n267 | detr.backbone.layer3.2.conv2                                 | Conv2d                  | 589 K \n268 | detr.backbone.layer3.2.bn2                                   | BatchNorm2d             | 512   \n269 | detr.backbone.layer3.2.conv3                                 | Conv2d                  | 262 K \n270 | detr.backbone.layer3.2.bn3                                   | BatchNorm2d             | 2 K   \n271 | detr.backbone.layer3.2.relu                                  | ReLU                    | 0     \n272 | detr.backbone.layer3.3                                       | Bottleneck              | 1 M   \n273 | detr.backbone.layer3.3.conv1                                 | Conv2d                  | 262 K \n274 | detr.backbone.layer3.3.bn1                                   | BatchNorm2d             | 512   \n275 | detr.backbone.layer3.3.conv2                                 | Conv2d                  | 589 K \n276 | detr.backbone.layer3.3.bn2                                   | BatchNorm2d             | 512   \n277 | detr.backbone.layer3.3.conv3                                 | Conv2d                  | 262 K \n278 | detr.backbone.layer3.3.bn3                                   | BatchNorm2d             | 2 K   \n279 | detr.backbone.layer3.3.relu                                  | ReLU                    | 0     \n280 | detr.backbone.layer3.4                                       | Bottleneck              | 1 M   \n281 | detr.backbone.layer3.4.conv1                                 | Conv2d                  | 262 K \n282 | detr.backbone.layer3.4.bn1                                   | BatchNorm2d             | 512   \n283 | detr.backbone.layer3.4.conv2                                 | Conv2d                  | 589 K \n284 | detr.backbone.layer3.4.bn2                                   | BatchNorm2d             | 512   \n285 | detr.backbone.layer3.4.conv3                                 | Conv2d                  | 262 K \n286 | detr.backbone.layer3.4.bn3                                   | BatchNorm2d             | 2 K   \n287 | detr.backbone.layer3.4.relu                                  | ReLU                    | 0     \n288 | detr.backbone.layer3.5                                       | Bottleneck              | 1 M   \n289 | detr.backbone.layer3.5.conv1                                 | Conv2d                  | 262 K \n290 | detr.backbone.layer3.5.bn1                                   | BatchNorm2d             | 512   \n291 | detr.backbone.layer3.5.conv2                                 | Conv2d                  | 589 K \n292 | detr.backbone.layer3.5.bn2                                   | BatchNorm2d             | 512   \n293 | detr.backbone.layer3.5.conv3                                 | Conv2d                  | 262 K \n294 | detr.backbone.layer3.5.bn3                                   | BatchNorm2d             | 2 K   \n295 | detr.backbone.layer3.5.relu                                  | ReLU                    | 0     \n296 | detr.backbone.layer4                                         | Sequential              | 14 M  \n297 | detr.backbone.layer4.0                                       | Bottleneck              | 6 M   \n298 | detr.backbone.layer4.0.conv1                                 | Conv2d                  | 524 K \n299 | detr.backbone.layer4.0.bn1                                   | BatchNorm2d             | 1 K   \n300 | detr.backbone.layer4.0.conv2                                 | Conv2d                  | 2 M   \n301 | detr.backbone.layer4.0.bn2                                   | BatchNorm2d             | 1 K   \n302 | detr.backbone.layer4.0.conv3                                 | Conv2d                  | 1 M   \n303 | detr.backbone.layer4.0.bn3                                   | BatchNorm2d             | 4 K   \n304 | detr.backbone.layer4.0.relu                                  | ReLU                    | 0     \n305 | detr.backbone.layer4.0.downsample                            | Sequential              | 2 M   \n306 | detr.backbone.layer4.0.downsample.0                          | Conv2d                  | 2 M   \n307 | detr.backbone.layer4.0.downsample.1                          | BatchNorm2d             | 4 K   \n308 | detr.backbone.layer4.1                                       | Bottleneck              | 4 M   \n309 | detr.backbone.layer4.1.conv1                                 | Conv2d                  | 1 M   \n310 | detr.backbone.layer4.1.bn1                                   | BatchNorm2d             | 1 K   \n311 | detr.backbone.layer4.1.conv2                                 | Conv2d                  | 2 M   \n312 | detr.backbone.layer4.1.bn2                                   | BatchNorm2d             | 1 K   \n313 | detr.backbone.layer4.1.conv3                                 | Conv2d                  | 1 M   \n314 | detr.backbone.layer4.1.bn3                                   | BatchNorm2d             | 4 K   \n315 | detr.backbone.layer4.1.relu                                  | ReLU                    | 0     \n316 | detr.backbone.layer4.2                                       | Bottleneck              | 4 M   \n317 | detr.backbone.layer4.2.conv1                                 | Conv2d                  | 1 M   \n318 | detr.backbone.layer4.2.bn1                                   | BatchNorm2d             | 1 K   \n319 | detr.backbone.layer4.2.conv2                                 | Conv2d                  | 2 M   \n320 | detr.backbone.layer4.2.bn2                                   | BatchNorm2d             | 1 K   \n321 | detr.backbone.layer4.2.conv3                                 | Conv2d                  | 1 M   \n322 | detr.backbone.layer4.2.bn3                                   | BatchNorm2d             | 4 K   \n323 | detr.backbone.layer4.2.relu                                  | ReLU                    | 0     \n324 | detr.backbone.avgpool                                        | AdaptiveAvgPool2d       | 0     \n325 | detr.conv                                                    | Conv2d                  | 524 K \n326 | detr.transformer                                             | Transformer             | 17 M  \n327 | detr.transformer.encoder                                     | TransformerEncoder      | 7 M   \n328 | detr.transformer.encoder.layers                              | ModuleList              | 7 M   \n329 | detr.transformer.encoder.layers.0                            | TransformerEncoderLayer | 1 M   \n330 | detr.transformer.encoder.layers.0.self_attn                  | MultiheadAttention      | 263 K \n331 | detr.transformer.encoder.layers.0.self_attn.out_proj         | Linear                  | 65 K  \n332 | detr.transformer.encoder.layers.0.linear1                    | Linear                  | 526 K \n333 | detr.transformer.encoder.layers.0.dropout                    | Dropout                 | 0     \n334 | detr.transformer.encoder.layers.0.linear2                    | Linear                  | 524 K \n335 | detr.transformer.encoder.layers.0.norm1                      | LayerNorm               | 512   \n336 | detr.transformer.encoder.layers.0.norm2                      | LayerNorm               | 512   \n337 | detr.transformer.encoder.layers.0.dropout1                   | Dropout                 | 0     \n338 | detr.transformer.encoder.layers.0.dropout2                   | Dropout                 | 0     \n339 | detr.transformer.encoder.layers.1                            | TransformerEncoderLayer | 1 M   \n340 | detr.transformer.encoder.layers.1.self_attn                  | MultiheadAttention      | 263 K \n341 | detr.transformer.encoder.layers.1.self_attn.out_proj         | Linear                  | 65 K  \n342 | detr.transformer.encoder.layers.1.linear1                    | Linear                  | 526 K \n343 | detr.transformer.encoder.layers.1.dropout                    | Dropout                 | 0     \n344 | detr.transformer.encoder.layers.1.linear2                    | Linear                  | 524 K \n345 | detr.transformer.encoder.layers.1.norm1                      | LayerNorm               | 512   \n346 | detr.transformer.encoder.layers.1.norm2                      | LayerNorm               | 512   \n347 | detr.transformer.encoder.layers.1.dropout1                   | Dropout                 | 0     \n348 | detr.transformer.encoder.layers.1.dropout2                   | Dropout                 | 0     \n349 | detr.transformer.encoder.layers.2                            | TransformerEncoderLayer | 1 M   \n350 | detr.transformer.encoder.layers.2.self_attn                  | MultiheadAttention      | 263 K \n351 | detr.transformer.encoder.layers.2.self_attn.out_proj         | Linear                  | 65 K  \n352 | detr.transformer.encoder.layers.2.linear1                    | Linear                  | 526 K \n353 | detr.transformer.encoder.layers.2.dropout                    | Dropout                 | 0     \n354 | detr.transformer.encoder.layers.2.linear2                    | Linear                  | 524 K \n355 | detr.transformer.encoder.layers.2.norm1                      | LayerNorm               | 512   \n356 | detr.transformer.encoder.layers.2.norm2                      | LayerNorm               | 512   \n357 | detr.transformer.encoder.layers.2.dropout1                   | Dropout                 | 0     \n358 | detr.transformer.encoder.layers.2.dropout2                   | Dropout                 | 0     \n359 | detr.transformer.encoder.layers.3                            | TransformerEncoderLayer | 1 M   \n360 | detr.transformer.encoder.layers.3.self_attn                  | MultiheadAttention      | 263 K \n361 | detr.transformer.encoder.layers.3.self_attn.out_proj         | Linear                  | 65 K  \n362 | detr.transformer.encoder.layers.3.linear1                    | Linear                  | 526 K \n363 | detr.transformer.encoder.layers.3.dropout                    | Dropout                 | 0     \n364 | detr.transformer.encoder.layers.3.linear2                    | Linear                  | 524 K \n365 | detr.transformer.encoder.layers.3.norm1                      | LayerNorm               | 512   \n366 | detr.transformer.encoder.layers.3.norm2                      | LayerNorm               | 512   \n367 | detr.transformer.encoder.layers.3.dropout1                   | Dropout                 | 0     \n368 | detr.transformer.encoder.layers.3.dropout2                   | Dropout                 | 0     \n369 | detr.transformer.encoder.layers.4                            | TransformerEncoderLayer | 1 M   \n370 | detr.transformer.encoder.layers.4.self_attn                  | MultiheadAttention      | 263 K \n371 | detr.transformer.encoder.layers.4.self_attn.out_proj         | Linear                  | 65 K  \n372 | detr.transformer.encoder.layers.4.linear1                    | Linear                  | 526 K \n373 | detr.transformer.encoder.layers.4.dropout                    | Dropout                 | 0     \n374 | detr.transformer.encoder.layers.4.linear2                    | Linear                  | 524 K \n375 | detr.transformer.encoder.layers.4.norm1                      | LayerNorm               | 512   \n376 | detr.transformer.encoder.layers.4.norm2                      | LayerNorm               | 512   \n377 | detr.transformer.encoder.layers.4.dropout1                   | Dropout                 | 0     \n378 | detr.transformer.encoder.layers.4.dropout2                   | Dropout                 | 0     \n379 | detr.transformer.encoder.layers.5                            | TransformerEncoderLayer | 1 M   \n380 | detr.transformer.encoder.layers.5.self_attn                  | MultiheadAttention      | 263 K \n381 | detr.transformer.encoder.layers.5.self_attn.out_proj         | Linear                  | 65 K  \n382 | detr.transformer.encoder.layers.5.linear1                    | Linear                  | 526 K \n383 | detr.transformer.encoder.layers.5.dropout                    | Dropout                 | 0     \n384 | detr.transformer.encoder.layers.5.linear2                    | Linear                  | 524 K \n385 | detr.transformer.encoder.layers.5.norm1                      | LayerNorm               | 512   \n386 | detr.transformer.encoder.layers.5.norm2                      | LayerNorm               | 512   \n387 | detr.transformer.encoder.layers.5.dropout1                   | Dropout                 | 0     \n388 | detr.transformer.encoder.layers.5.dropout2                   | Dropout                 | 0     \n389 | detr.transformer.encoder.norm                                | LayerNorm               | 512   \n390 | detr.transformer.decoder                                     | TransformerDecoder      | 9 M   \n391 | detr.transformer.decoder.layers                              | ModuleList              | 9 M   \n392 | detr.transformer.decoder.layers.0                            | TransformerDecoderLayer | 1 M   \n393 | detr.transformer.decoder.layers.0.self_attn                  | MultiheadAttention      | 263 K \n394 | detr.transformer.decoder.layers.0.self_attn.out_proj         | Linear                  | 65 K  \n395 | detr.transformer.decoder.layers.0.multihead_attn             | MultiheadAttention      | 263 K \n396 | detr.transformer.decoder.layers.0.multihead_attn.out_proj    | Linear                  | 65 K  \n397 | detr.transformer.decoder.layers.0.linear1                    | Linear                  | 526 K \n398 | detr.transformer.decoder.layers.0.dropout                    | Dropout                 | 0     \n399 | detr.transformer.decoder.layers.0.linear2                    | Linear                  | 524 K \n400 | detr.transformer.decoder.layers.0.norm1                      | LayerNorm               | 512   \n401 | detr.transformer.decoder.layers.0.norm2                      | LayerNorm               | 512   \n402 | detr.transformer.decoder.layers.0.norm3                      | LayerNorm               | 512   \n403 | detr.transformer.decoder.layers.0.dropout1                   | Dropout                 | 0     \n404 | detr.transformer.decoder.layers.0.dropout2                   | Dropout                 | 0     \n405 | detr.transformer.decoder.layers.0.dropout3                   | Dropout                 | 0     \n406 | detr.transformer.decoder.layers.1                            | TransformerDecoderLayer | 1 M   \n407 | detr.transformer.decoder.layers.1.self_attn                  | MultiheadAttention      | 263 K \n408 | detr.transformer.decoder.layers.1.self_attn.out_proj         | Linear                  | 65 K  \n409 | detr.transformer.decoder.layers.1.multihead_attn             | MultiheadAttention      | 263 K \n410 | detr.transformer.decoder.layers.1.multihead_attn.out_proj    | Linear                  | 65 K  \n411 | detr.transformer.decoder.layers.1.linear1                    | Linear                  | 526 K \n412 | detr.transformer.decoder.layers.1.dropout                    | Dropout                 | 0     \n413 | detr.transformer.decoder.layers.1.linear2                    | Linear                  | 524 K \n414 | detr.transformer.decoder.layers.1.norm1                      | LayerNorm               | 512   \n415 | detr.transformer.decoder.layers.1.norm2                      | LayerNorm               | 512   \n416 | detr.transformer.decoder.layers.1.norm3                      | LayerNorm               | 512   \n417 | detr.transformer.decoder.layers.1.dropout1                   | Dropout                 | 0     \n418 | detr.transformer.decoder.layers.1.dropout2                   | Dropout                 | 0     \n419 | detr.transformer.decoder.layers.1.dropout3                   | Dropout                 | 0     \n420 | detr.transformer.decoder.layers.2                            | TransformerDecoderLayer | 1 M   \n421 | detr.transformer.decoder.layers.2.self_attn                  | MultiheadAttention      | 263 K \n422 | detr.transformer.decoder.layers.2.self_attn.out_proj         | Linear                  | 65 K  \n423 | detr.transformer.decoder.layers.2.multihead_attn             | MultiheadAttention      | 263 K \n424 | detr.transformer.decoder.layers.2.multihead_attn.out_proj    | Linear                  | 65 K  \n425 | detr.transformer.decoder.layers.2.linear1                    | Linear                  | 526 K \n426 | detr.transformer.decoder.layers.2.dropout                    | Dropout                 | 0     \n427 | detr.transformer.decoder.layers.2.linear2                    | Linear                  | 524 K \n428 | detr.transformer.decoder.layers.2.norm1                      | LayerNorm               | 512   \n429 | detr.transformer.decoder.layers.2.norm2                      | LayerNorm               | 512   \n430 | detr.transformer.decoder.layers.2.norm3                      | LayerNorm               | 512   \n431 | detr.transformer.decoder.layers.2.dropout1                   | Dropout                 | 0     \n432 | detr.transformer.decoder.layers.2.dropout2                   | Dropout                 | 0     \n433 | detr.transformer.decoder.layers.2.dropout3                   | Dropout                 | 0     \n434 | detr.transformer.decoder.layers.3                            | TransformerDecoderLayer | 1 M   \n435 | detr.transformer.decoder.layers.3.self_attn                  | MultiheadAttention      | 263 K \n436 | detr.transformer.decoder.layers.3.self_attn.out_proj         | Linear                  | 65 K  \n437 | detr.transformer.decoder.layers.3.multihead_attn             | MultiheadAttention      | 263 K \n438 | detr.transformer.decoder.layers.3.multihead_attn.out_proj    | Linear                  | 65 K  \n439 | detr.transformer.decoder.layers.3.linear1                    | Linear                  | 526 K \n440 | detr.transformer.decoder.layers.3.dropout                    | Dropout                 | 0     \n441 | detr.transformer.decoder.layers.3.linear2                    | Linear                  | 524 K \n442 | detr.transformer.decoder.layers.3.norm1                      | LayerNorm               | 512   \n443 | detr.transformer.decoder.layers.3.norm2                      | LayerNorm               | 512   \n444 | detr.transformer.decoder.layers.3.norm3                      | LayerNorm               | 512   \n445 | detr.transformer.decoder.layers.3.dropout1                   | Dropout                 | 0     \n446 | detr.transformer.decoder.layers.3.dropout2                   | Dropout                 | 0     \n447 | detr.transformer.decoder.layers.3.dropout3                   | Dropout                 | 0     \n448 | detr.transformer.decoder.layers.4                            | TransformerDecoderLayer | 1 M   \n449 | detr.transformer.decoder.layers.4.self_attn                  | MultiheadAttention      | 263 K \n450 | detr.transformer.decoder.layers.4.self_attn.out_proj         | Linear                  | 65 K  \n451 | detr.transformer.decoder.layers.4.multihead_attn             | MultiheadAttention      | 263 K \n452 | detr.transformer.decoder.layers.4.multihead_attn.out_proj    | Linear                  | 65 K  \n453 | detr.transformer.decoder.layers.4.linear1                    | Linear                  | 526 K \n454 | detr.transformer.decoder.layers.4.dropout                    | Dropout                 | 0     \n455 | detr.transformer.decoder.layers.4.linear2                    | Linear                  | 524 K \n456 | detr.transformer.decoder.layers.4.norm1                      | LayerNorm               | 512   \n457 | detr.transformer.decoder.layers.4.norm2                      | LayerNorm               | 512   \n458 | detr.transformer.decoder.layers.4.norm3                      | LayerNorm               | 512   \n459 | detr.transformer.decoder.layers.4.dropout1                   | Dropout                 | 0     \n460 | detr.transformer.decoder.layers.4.dropout2                   | Dropout                 | 0     \n461 | detr.transformer.decoder.layers.4.dropout3                   | Dropout                 | 0     \n462 | detr.transformer.decoder.layers.5                            | TransformerDecoderLayer | 1 M   \n463 | detr.transformer.decoder.layers.5.self_attn                  | MultiheadAttention      | 263 K \n464 | detr.transformer.decoder.layers.5.self_attn.out_proj         | Linear                  | 65 K  \n465 | detr.transformer.decoder.layers.5.multihead_attn             | MultiheadAttention      | 263 K \n466 | detr.transformer.decoder.layers.5.multihead_attn.out_proj    | Linear                  | 65 K  \n467 | detr.transformer.decoder.layers.5.linear1                    | Linear                  | 526 K \n468 | detr.transformer.decoder.layers.5.dropout                    | Dropout                 | 0     \n469 | detr.transformer.decoder.layers.5.linear2                    | Linear                  | 524 K \n470 | detr.transformer.decoder.layers.5.norm1                      | LayerNorm               | 512   \n471 | detr.transformer.decoder.layers.5.norm2                      | LayerNorm               | 512   \n472 | detr.transformer.decoder.layers.5.norm3                      | LayerNorm               | 512   \n473 | detr.transformer.decoder.layers.5.dropout1                   | Dropout                 | 0     \n474 | detr.transformer.decoder.layers.5.dropout2                   | Dropout                 | 0     \n475 | detr.transformer.decoder.layers.5.dropout3                   | Dropout                 | 0     \n476 | detr.transformer.decoder.norm                                | LayerNorm               | 512   \n477 | detr.linear_class                                            | Linear                  | 23 K  \n478 | detr.linear_bbox                                             | Linear                  | 1 K   \n479 | classifier                                                   | Linear                  | 83 M  \n480 | drop_out                                                     | Dropout                 | 0     \n481 | log_softmax                                                  | LogSoftmax              | 0     \n","name":"stderr"},{"output_type":"display_data","data":{"text/plain":"HBox(children=(FloatProgress(value=1.0, bar_style='info', description='Training', layout=Layout(flex='2'), max…","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"279c1c3ab963431f92a9ddaedced3ecb"}},"metadata":{}},{"output_type":"stream","text":"/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:138: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:138: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:138: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:138: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:138: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:138: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:138: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:138: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","name":"stderr"},{"output_type":"stream","text":"(640, 512)\n(640, 512, 3)\n(508, 640)\n(508, 640, 3)\n","name":"stdout"},{"output_type":"stream","text":"/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:39: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n","name":"stderr"},{"output_type":"stream","text":"(640, 514)\n(640, 514, 3)\n(494, 640)\n(494, 640, 3)\n(640, 512)\n(640, 512, 3)\n(490, 640)\n(490, 640, 3)\n(482, 640)\n(482, 640, 3)\n(640, 512)\n(640, 512, 3)\n(514, 640)\n(514, 640, 3)\n(504, 640)\n(504, 640, 3)\n(513, 640)\n(513, 640, 3)\n(466, 640)\n(466, 640, 3)\n(501, 640)\n(501, 640, 3)\n(640, 632)\n(640, 632, 3)\n","name":"stdout"},{"output_type":"stream","text":"/opt/conda/lib/python3.7/site-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:138: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:138: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:138: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:138: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:138: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:138: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:138: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:138: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","name":"stderr"},{"output_type":"stream","text":"(640, 515)\n(640, 515, 3)\n(508, 640)\n(508, 640, 3)\n(428, 640)\n(428, 640, 3)\n(498, 640)\n(498, 640, 3)\n(508, 640)\n(508, 640, 3)\n(510, 640)\n(510, 640, 3)\n(640, 510)\n(640, 510, 3)\n(640, 430)\n(640, 430, 3)\n(402, 640)\n(402, 640, 3)\n(640, 512)\n(640, 512, 3)\n(508, 640)\n(508, 640, 3)\n(640, 512)\n(640, 512, 3)\n(508, 640)\n(508, 640, 3)\n(509, 640)\n(509, 640, 3)\n(394, 640)\n(394, 640, 3)\n(640, 514)\n(640, 514, 3)\n(467, 640)\n(467, 640, 3)\n(489, 640)\n(489, 640, 3)\n(414, 640)\n(414, 640, 3)\n(508, 640)\n(508, 640, 3)\n(503, 640)\n(503, 640, 3)\n(404, 640)\n(404, 640, 3)\n","name":"stdout"},{"output_type":"stream","text":"/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:138: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:138: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:138: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:138: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:138: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:138: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:138: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:138: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","name":"stderr"},{"output_type":"stream","text":"(627, 640)\n(627, 640, 3)\n(395, 640)\n(395, 640, 3)\n(508, 640)\n(508, 640, 3)\n","name":"stdout"}]},{"metadata":{"id":"mpsKfzoTPvN9","outputId":"525e0c17-ba8b-466b-d06f-4492603a75c1","trusted":true},"cell_type":"code","source":["# Start tensorboard.\n","%reload_ext tensorboard\n","%tensorboard --logdir kaggle/working/lightning_logs "],"execution_count":null,"outputs":[]},{"metadata":{"id":"Lkixu9cEiuH4","trusted":false},"cell_type":"code","source":["model_save= vqa_detr"],"execution_count":null,"outputs":[]},{"metadata":{"id":"vmkzyZD2lms4","trusted":false},"cell_type":"code","source":["model_save.eval().to('cpu')"],"execution_count":null,"outputs":[]},{"metadata":{"id":"LocAAdSLiyoJ","outputId":"028c6b0f-d539-417e-d884-692b93bf0ac9","trusted":false},"cell_type":"code","source":["img,q,a=vqa_data[24]"],"execution_count":null,"outputs":[]},{"metadata":{"id":"M0C9hbUzjA7j","outputId":"07631c7f-d64e-43be-e593-ef7fe28bfddd","trusted":false},"cell_type":"code","source":["\n","out = model_save(img.unsqueeze(0),q['ids'].unsqueeze(0))\n"],"execution_count":null,"outputs":[]},{"metadata":{"id":"p672z_Qvjh2l","trusted":false},"cell_type":"code","source":["a,index = out['logits'].max(dim = -1)"],"execution_count":null,"outputs":[]},{"metadata":{"id":"lB5ULieHlM7I","outputId":"30b263f8-2f36-4c62-c944-d17fa6ecfa6f","trusted":false},"cell_type":"code","source":["bert_tokenizer.convert_ids_to_tokens(q['ids'])"],"execution_count":null,"outputs":[]},{"metadata":{"id":"0bvtBHGVkDr5","outputId":"57c04b54-e2f8-4b89-aa2f-18486a138ddc","trusted":false},"cell_type":"code","source":["all_answers[int(index)]"],"execution_count":null,"outputs":[]},{"metadata":{"id":"p2RgCuljj0rA","outputId":"0a969671-b7ac-4bf3-d9f6-9c6323dba0a7","trusted":false},"cell_type":"code","source":["plt.imshow(img.permute(1,2,0))"],"execution_count":null,"outputs":[]},{"metadata":{"id":"hYwwjn7cg5LL","outputId":"4f9ea055-089a-4e73-d727-40623c891fda","trusted":false},"cell_type":"code","source":["while 1:\n","    continue"],"execution_count":null,"outputs":[]},{"metadata":{"id":"jzGeoINdgUkg","trusted":false},"cell_type":"code","source":["import torch\n","from torch import nn"],"execution_count":null,"outputs":[]},{"metadata":{"id":"xeUiwvNxg5IL","outputId":"d016ae3b-bdb4-43a4-feaf-f5e9b673f147","trusted":false},"cell_type":"code","source":["loss = nn.CrossEntropyLoss()\n","input = torch.randn(3, 5, requires_grad=True)\n","target = torch.empty(3, dtype=torch.long).random_(5)\n","output = loss(input, target)\n","print(output)\n","output.backward()"],"execution_count":null,"outputs":[]},{"metadata":{"id":"zxWysSRHg5GM","outputId":"4597a5b0-57e0-48e6-9fd2-63346294fce5","trusted":false},"cell_type":"code","source":["torch.__version__"],"execution_count":null,"outputs":[]},{"metadata":{"id":"f7E47VdQBYim","outputId":"740f2c14-14aa-4500-e039-bfda773c20a5","trusted":false},"cell_type":"code","source":["output.requires_grad"],"execution_count":null,"outputs":[]},{"metadata":{"id":"9kRrRu0Pg5CD","outputId":"fa93fa9f-f554-46bf-8eb9-2f037c22789a","trusted":false},"cell_type":"code","source":["x = np.array([1, 2])\n","print(x.shape)"],"execution_count":null,"outputs":[]},{"metadata":{"id":"KB1c-xPkg4_7","trusted":false},"cell_type":"code","source":["z=np.expand_dims(x, axis=-1)\n","np.repeat(z,3,axis=-1).shape"],"execution_count":null,"outputs":[]},{"metadata":{"id":"HC7x-7cgg48D","outputId":"eceb7049-1453-49c7-ab64-298391547a3d","trusted":false},"cell_type":"code","source":[],"execution_count":null,"outputs":[]},{"metadata":{"id":"jMYEVVN6g46I","trusted":false},"cell_type":"code","source":[],"execution_count":null,"outputs":[]},{"metadata":{"id":"eW4yYVKkg42L","trusted":false},"cell_type":"code","source":[],"execution_count":null,"outputs":[]},{"metadata":{"id":"jN4YyXjspxbR","trusted":false},"cell_type":"code","source":["for im,q,a in data_loader:\n","    print(im.shape)\n","    print(q['ids'].shape)\n","    print(a.shape)\n","    break"],"execution_count":null,"outputs":[]},{"metadata":{"id":"AdEvUm4kDjvx","outputId":"5d64ecf0-9113-4ed4-884d-ab672c7a98ce","trusted":false},"cell_type":"code","source":["len(answer_to_index)"],"execution_count":null,"outputs":[]},{"metadata":{"id":"-EX7BA0oNFVc","trusted":false},"cell_type":"code","source":["img,q,a=vqa_data[58]"],"execution_count":null,"outputs":[]},{"metadata":{"id":"OJExIYdDGwlC","trusted":false},"cell_type":"code","source":["a,b=torch.max(a.unsqueeze(0),dim = 1)"],"execution_count":null,"outputs":[]},{"metadata":{"id":"D9ad7adt54T7","outputId":"44a748ef-865c-4ba6-d8a4-710325d74af6","trusted":false},"cell_type":"code","source":["b.requires_grad = True"],"execution_count":null,"outputs":[]},{"metadata":{"id":"72e-rwiQNFLF","outputId":"fbe9d96a-5881-4e41-dc2a-12963ab48a17","trusted":false},"cell_type":"code","source":["a.unsqueeze(0).view(-1,1).shape\n"],"execution_count":null,"outputs":[]},{"metadata":{"id":"A0R64hJAnvC4","trusted":false},"cell_type":"code","source":["bert_config = BertConfig(hidden_size=256, num_attention_heads=8, num_hidden_layers=6)\n","bert_decoder_config = BertConfig(is_decoder=True, hidden_size=256, num_attention_heads=8, num_hidden_layers=6)\n","\n","enc_dec_config = EncoderDecoderConfig.from_encoder_decoder_configs(encoder_config= bert_config, decoder_config= bert_decoder_config)\n"],"execution_count":null,"outputs":[]},{"metadata":{"id":"TcNZWDKPlRlN","trusted":false},"cell_type":"code","source":["model = EncoderDecoderModel(config= enc_dec_config)"],"execution_count":null,"outputs":[]},{"metadata":{"id":"pfZOXxIFlZK9","trusted":false},"cell_type":"code","source":["outputs = model(input_ids = q['ids'].unsqueeze(0), decoder_inputs_embeds = h[0].unsqueeze(0).flatten(2).permute(2, 0, 1))"],"execution_count":null,"outputs":[]},{"metadata":{"id":"KsjQtkw0mGv_","trusted":false},"cell_type":"code","source":["outputs[2].shape"],"execution_count":null,"outputs":[]},{"metadata":{"id":"w8H-1gtoRmWI","trusted":false},"cell_type":"code","source":["bert_decoder = BertModel(config  = bert_decoder_config)\n","outputs = bert_decoder(input_ids = q['ids'].unsqueeze(0), encoder_hidden_states = h[0].unsqueeze(0).flatten(2).permute(0,2,1))"],"execution_count":null,"outputs":[]},{"metadata":{"id":"9fpRcCBhSe63","trusted":false},"cell_type":"code","source":["outputs = bert_decoder(input_ids = q['ids'].unsqueeze(0), encoder_hidden_states = h[0].unsqueeze(0).flatten(2).permute(0,2,1))"],"execution_count":null,"outputs":[]},{"metadata":{"id":"b6q0BavU9BHN","trusted":false},"cell_type":"code","source":["while 1 :\n","    continue"],"execution_count":null,"outputs":[]},{"metadata":{"id":"-91LWQZDc1wK","trusted":false},"cell_type":"code","source":["BertModel??"],"execution_count":null,"outputs":[]},{"metadata":{"id":"rlIue81hy08k","trusted":false},"cell_type":"code","source":[" h[0].unsqueeze(0).flatten(2).permute(0, 2, 1).shape"],"execution_count":null,"outputs":[]},{"metadata":{"id":"TYLN4RJRXXuZ","trusted":false},"cell_type":"code","source":["q['ids'].unsqueeze(0).shape"],"execution_count":null,"outputs":[]},{"metadata":{"id":"SiLAyykeXx_G","trusted":false},"cell_type":"code","source":[],"execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.7.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":4}